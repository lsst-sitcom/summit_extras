{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74115f2c-5259-4f1d-a759-91b87f5bd9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a range of dayobs values to search - \n",
    "#day_obs_min = \"Today\"\n",
    "day_obs_min = \"2024-11-24\"\n",
    "#day_obs_max = \"Today\"\n",
    "day_obs_max = \"2024-11-24\"\n",
    "time_order = 'newest first'\n",
    "show_salIndex = 'all'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ac8228-7002-4ed1-b23f-c1563e5723e2",
   "metadata": {},
   "source": [
    "# EFD Scripts + Logs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d21c3a-158c-4713-b704-b974d1f80644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import option_context\n",
    "from IPython.display import display, Markdown, HTML\n",
    "from astropy.time import Time, TimeDelta\n",
    "import datetime\n",
    "import astropy.units as u\n",
    "import yaml\n",
    "import html\n",
    "\n",
    "# To generate a tiny gap in time\n",
    "EPS_TIME = np.timedelta64(1, 'ms')\n",
    "TIMESTAMP_ZERO = Time(0, format='unix_tai').utc.datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d01e192-a02c-4900-9bfb-5d50e8bb8ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "# lsst-ts-xml is in conda\n",
    "# See https://github.com/lsst-ts/ts_xml/blob/develop/python/lsst/ts/xml/enums \n",
    "from lsst.ts.xml.sal_enums import State as CSCState\n",
    "from lsst.ts.xml.enums.ScriptQueue import ScriptProcessState, SalIndex\n",
    "from lsst.ts.xml.enums.Script import ScriptState\n",
    "from lsst.ts.xml.enums.Watcher import AlarmSeverity\n",
    "\n",
    "# Run as 'apply' per row (axis=1)\n",
    "def apply_enum(x: pd.Series, column: str, enumvals: Enum) -> str:\n",
    "    return enumvals(x[column]).name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b0abe2-3554-4afd-84cf-e28067008911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from lsst_efd_client import EfdClient\n",
    "\n",
    "try:\n",
    "    from lsst.summit.utils import ConsDbClient\n",
    "    has_summit_utils = True\n",
    "except ImportError:\n",
    "    # No lsst.summit.utils\n",
    "    has_summit_utils = False\n",
    "\n",
    "try:\n",
    "    from lsst.rsp import get_access_token\n",
    "except ImportError:\n",
    "    def get_access_token(token_file : str | None = None) -> str:\n",
    "        token = os.environ.get(\"ACCESS_TOKEN\")\n",
    "        if token == None:\n",
    "            if token_file is not None:\n",
    "                with open(\"token_file\", \"r\") as f:\n",
    "                    token = f.read()\n",
    "            else:\n",
    "                warnings.warn(\"No RSP token available.\")\n",
    "        return token\n",
    "\n",
    "\n",
    "def get_clients() -> dict:\n",
    "    \"\"\"Return site-specific client connections. \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    endpoints : `dict`\n",
    "        Dictionary with `efd`, `obsenv`, \n",
    "        `narrative_log`, and `exposure_log`\n",
    "        connection information.\n",
    "        For the obsenv, narrative log and exposure log, these are only\n",
    "        defined for the summit or USDF.\n",
    "\n",
    "    Note\n",
    "    ----\n",
    "    The authentication token required to access the log services\n",
    "    is an RSP token, and is RSP site-specific. \n",
    "    For users outside the RSP, a token can be created as described in\n",
    "    https://nb.lsst.io/environment/tokens.html\n",
    "    \"\"\"\n",
    "    # Set up authentication\n",
    "    token = get_access_token()\n",
    "    auth = (\"user\", token)\n",
    "    # This authentication is for nightlog, exposurelog, nightreport currently\n",
    "    # But I think it's the same underlying info for EfdClient i.e.\n",
    "    # https://github.com/lsst/schedview/blob/e11fbd51ee5e22d11fef9a52f66dfcc082181cb6/schedview/app/scheduler_dashboard/influxdb_client.py\n",
    "    # For lots more information on rubin tokens see DMTN-234.\n",
    "    # For information on scopes, see DMTN-235.\n",
    "    \n",
    "    # let's do this like lsst.summit.utils.getSite but simpler\n",
    "    site = \"UNKNOWN\"\n",
    "    location = os.getenv(\"EXTERNAL_INSTANCE_URL\", \"\")\n",
    "    if \"tucson-teststand\" in location:\n",
    "        site = \"tucson\"\n",
    "    elif \"summit-lsp\" in location:\n",
    "        site = \"summit\"\n",
    "    elif \"base-lsp\" in location:\n",
    "        site = \"base\"\n",
    "    elif \"usdf-rsp\" in location:\n",
    "        site = \"usdf\"\n",
    "    # If location not set, next step is to check hostname\n",
    "    elif location == \"\":\n",
    "        hostname = os.getenv(\"HOSTNAME\", \"\")\n",
    "        interactiveNodes = (\"sdfrome\", \"sdfiana\")\n",
    "        if hostname.startswith(interactiveNodes):\n",
    "            site = \"usdf\"\n",
    "        elif hostname == \"htcondor.ls.lsst.org\":\n",
    "            site = \"base\"\n",
    "        elif hostname == \"htcondor.cp.lsst.org\":\n",
    "            site = \"summit\"\n",
    "    # If none of the above, use usdf again.\n",
    "    if site == \"UNKNOWN\":\n",
    "        site = \"usdf\"\n",
    "    \n",
    "    if site == \"summit\":\n",
    "        api_base = \"https://summit-lsp.lsst.codes\"\n",
    "        efd_client = EfdClient(\"summit_efd\")\n",
    "        obsenv_client = EfdClient(\"summit_efd\", db_name=\"lsst.obsenv\")\n",
    "    elif site == \"tucson\":\n",
    "        api_base = None\n",
    "        efd_client = EfdClient(\"tucson_teststand_efd\")\n",
    "        obsenv_client = EfdClient(\"tucson_teststand_efd\", db_name=\"lsst.obsenv\")\n",
    "    elif site == \"base\":\n",
    "        api_base = \"https://base-lsp.slac.lsst.codes\"\n",
    "        efd_client = EfdClient(\"base_efd\")\n",
    "        obsenv_client = EfdClient(\"base_efd\", db_name=\"lsst.obsenv\")\n",
    "    elif site == \"usdf\":\n",
    "        # For tokens, need to distinguish between dev and prod\n",
    "        if \"dev\" in location:\n",
    "            api_base = \"https://usdf-rsp-dev.slac.stanford.edu\"\n",
    "        else:\n",
    "            api_base = \"https://usdf-rsp.slac.stanford.edu\"\n",
    "        efd_client = EfdClient(\"usdf_efd\")\n",
    "        obsenv_client = EfdClient(\"usdf_efd\", db_name='lsst.obsenv')\n",
    "    else:\n",
    "        # Assume USDF prod\n",
    "        efd_client = EfdClient(\"usdf_efd\")\n",
    "        obsenv_client = EfdClient(\"usdf_efd\", db_name='lsst.obsenv')\n",
    "        api_base = \"https://usdf-rsp.slac.stanford.edu\"\n",
    "    narrative_log =  \"/narrativelog/messages\"\n",
    "    exposure_log = \"/exposurelog/messages\"\n",
    "    nightreport =  \"/nightreport/reports\"\n",
    "\n",
    "    endpoints = {'api_base': api_base, 'auth': auth, \n",
    "                'efd': efd_client, 'obsenv': obsenv_client, \n",
    "                'narrative_log': narrative_log, 'exposure_log': exposure_log,  \n",
    "                'nightreport': nightreport}\n",
    "    \n",
    "    # If some verbose output is desired\n",
    "    # We'll put this here to make it easier to avoid printing the auth token\n",
    "    endpoints_string = f\"base url: {endpoints['api_base']} \" \n",
    "    endpoints_string += f\"efd host: {endpoints['efd'].influx_client.host}\"\n",
    "    endpoints['string'] = endpoints_string\n",
    "\n",
    "    return endpoints\n",
    "\n",
    "def query_logging_services(endpoint: str, auth: tuple, params: dict) -> pd.DataFrame:\n",
    "    \"\"\"Send query to narrative log or exposure log services.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    endpoint : `str`\n",
    "        The URL to send the query to.\n",
    "        Usually like `https://usdf-rsp.slac.stanford.edu/narrativelog/messages`\n",
    "    auth : `tuple`\n",
    "        The username and password for authentication.\n",
    "        The username can be any string, the password should be an RSP token.\n",
    "        See e.g. https://nb.lsst.io/environment/tokens.html \n",
    "    params : `dict`\n",
    "        Dictionary of parameters for the REST API query.\n",
    "        See docs for each service for more details\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    messages : `pd.DataFrame`\n",
    "        The returned log messages (if any available), in a dataframe.\n",
    "    \"\"\"\n",
    "    # Very often, requests from the logging endpoints fail the first time.\n",
    "    response = requests.get(endpoint, auth=auth, params=params)\n",
    "    # Try twice.\n",
    "    if response.status_code != 200:\n",
    "        response = requests.get(endpoint, auth=auth, params=params)\n",
    "    if response.status_code != 200:\n",
    "        err_string = f\"{endpoint} \"\n",
    "        err_string += \" unavailable.\"\n",
    "        print(err_string)\n",
    "        print(response)\n",
    "        print(response.status_code)\n",
    "        messages = []\n",
    "    else:\n",
    "        messages = response.json()\n",
    "    messages = pd.DataFrame(messages)\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c9f44b-5c40-49f0-834b-8b78ce785c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query any EFD topic for the timespan day_obs_min to day_obs_max, when you don't already know the fields\n",
    "# endpoints = get_clients()\n",
    "# efd_client = endpoints['efd']\n",
    "# topic = 'lsst.sal.ScriptQueue.logevent_summaryState'\n",
    "# fields = await efd_client.get_fields(topic)\n",
    "# fields = [f for f in fields if 'private' not in f and f != 'name' and f!= \"duration\"]\n",
    "# #dd = await efd_client.select_time_series(topic, fields, tstart, tend)\n",
    "# #or top 5 .. \n",
    "# dd = await efd_client.select_top_n(topic, fields, 5)\n",
    "# dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bc2c0c-0468-43ef-a276-2d1ac7763cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_script_stream(t_start: Time, t_end: Time, efd_client: EfdClient) -> pd.DataFrame:\n",
    "    \"\"\"Get script description and configuration from lsst.sal.Script.logevent_description\n",
    "    and lsst.sal.Script.command_configure topics.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    t_start : `astropy.Time`\n",
    "        The time to start searching for script events.\n",
    "    t_end : `astropy.Time`\n",
    "        The time at which to end searching for script events.\n",
    "    efd_client : `EfdClient`\n",
    "        EfdClient to query the efd.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    script_stream : `pd.DataFrame`\n",
    "        DataFrame containing script description and configuration.\n",
    "\n",
    "    Note\n",
    "    ----\n",
    "    Note that these do not explicitly carry the scriptqueue salindex information.\n",
    "    The \"salIndex\" in these topics is the script_salIndex. \n",
    "    \"\"\"\n",
    "    # Script will find information about how scripts are configured. \n",
    "    # The description topic gives a more succinct human name to the scripts\n",
    "    topic = 'lsst.sal.Script.logevent_description'\n",
    "    fields = ['classname', 'description', 'salIndex']\n",
    "    scriptdescription = await efd_client.select_time_series(topic, fields, t_start, t_end)\n",
    "    scriptdescription.rename({'salIndex': 'script_salIndex'}, axis=1, inplace=True)\n",
    "            \n",
    "    # This gets us more information about the script parameters, how they were configured\n",
    "    topic = 'lsst.sal.Script.command_configure'\n",
    "    fields = ['blockId', 'config',' executionId', 'salIndex']\n",
    "    fields = await efd_client.get_fields(topic)\n",
    "    fields = [f for f in fields if 'private' not in f]\n",
    "    # note blockId is only filled for JSON BLOCK activities\n",
    "    scriptconfig = await efd_client.select_time_series(topic, fields, t_start, t_end)\n",
    "    scriptconfig.rename({'salIndex': 'script_salIndex'}, axis=1, inplace=True)\n",
    "\n",
    "    # Merge these together on script_salIndex which is unique over tinterval\n",
    "    # Found that (command_configure - script description) index time is mostly << 1 second for each script and < 1 second over a night\n",
    "    if len(scriptconfig) == 0 or len(scriptdescription) == 0:\n",
    "        print(f\"Length of scriptdescription ({len(scriptdescription)}) and scriptconfig ({len(scriptconfig)}) in time period {t_start.utc.iso} to {t_end.utc.iso}\")\n",
    "        script_stream = pd.DataFrame([])\n",
    "    else:\n",
    "        script_stream = pd.merge(scriptdescription, scriptconfig, on='script_salIndex', suffixes=['_d', '_r'])\n",
    "    return script_stream\n",
    "\n",
    "\n",
    "async def get_script_state(t_start: Time, t_end: Time, queueIndex: int | None, efd_client: EfdClient) -> pd.DataFrame:\n",
    "    \"\"\"Get script status from lsst.sal.ScriptQueue.logevent_script topic.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    t_start : `astropy.Time`\n",
    "        The time to start searching for script events.\n",
    "    t_end : `astropy.Time`\n",
    "        The time at which to end searching for script events.\n",
    "    efd_client : `EfdClient`\n",
    "        EfdClient to query the efd.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    script_state : `pd.DataFrame`\n",
    "        DataFrame containing timing information and states.\n",
    "\n",
    "        \n",
    "    Note\n",
    "    ----\n",
    "    The scriptqueue is explicit here, in the salIndex. From here, these can be tied to \n",
    "    the running of individual scripts, within a single restart of the scriptqueue only.\n",
    "    \"\"\"\n",
    "    # The status of each of these scripts is stored in scriptQueue.logevent_script\n",
    "    # so find the status of each of these scripts (this is status at individual stages).\n",
    "    topic = 'lsst.sal.ScriptQueue.logevent_script'\n",
    "    fields = ['blockId', 'path', 'processState', 'scriptState', 'salIndex', 'scriptSalIndex', \n",
    "             'timestampProcessStart', 'timestampConfigureStart', 'timestampConfigureEnd', 'timestampRunStart', 'timestampProcessEnd']\n",
    "    # Providing an integer salIndex will restrict this query to a single queue, but None will query all queues.\n",
    "    scripts = await efd_client.select_time_series(topic, fields, t_start, t_end, index=queueIndex)\n",
    "    scripts.rename({'scriptSalIndex': 'script_salIndex'}, axis=1, inplace=True)\n",
    "    if len(scripts) == 0:\n",
    "        print(f\"Found 0 script events in {t_start.utc.iso} to {t_end.utc.iso}.\")\n",
    "        script_status = pd.DataFrame([])\n",
    "        \n",
    "    else:\n",
    "        # Group scripts on 'script_salIndex' to consolidate the information about its status stages\n",
    "        # Make a new column which we will fill with the max script state (== final state, given enum)\n",
    "        # (new column so we don't have to deal with multi-indexes from multiple aggregation methods)\n",
    "        scripts['finalScriptState'] = scripts['scriptState']\n",
    "        script_status = scripts.groupby('script_salIndex').agg({'path': 'first', \n",
    "                                                                  'salIndex': 'max', \n",
    "                                                                  'finalScriptState': 'max', \n",
    "                                                                  'scriptState': 'unique', \n",
    "                                                                  'processState': 'unique', \n",
    "                                                                  'timestampProcessStart': 'min', \n",
    "                                                                  'timestampConfigureStart': 'min', \n",
    "                                                                  'timestampConfigureEnd': 'max', \n",
    "                                                                  'timestampRunStart': 'max', \n",
    "                                                                  'timestampProcessEnd': 'max'})\n",
    "        # Convert timestamp columns from unix_tai timestamps for readability.\n",
    "        # Yes, these timestamps really are unix_tai. \n",
    "        for col in [c for c in script_status.columns if c.startswith('timestamp')]:\n",
    "            script_status[col] = Time(script_status[col], format='unix_tai').utc.datetime\n",
    "        # Apply ScriptState enum for readability of final state\n",
    "        script_status['finalScriptState'] = script_status.apply(apply_enum, args=['finalScriptState', ScriptState], axis=1)\n",
    "        # Will apply 'best time' index after merge with script_stream\n",
    "    return script_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acdbdab-df7f-43fc-a678-e0039a33a810",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_script_status(t_start: Time, t_end: Time, efd_client: EfdClient) -> pd.DataFrame:\n",
    "    \"\"\"Given a start and end time, appropriately query each ScriptQueue to find \n",
    "    script descriptions, configurations and status.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    t_start : `astropy.Time`\n",
    "        The time to start searching for script events.\n",
    "    t_end : `astropy.Time`\n",
    "        The time at which to end searching for script events.\n",
    "    efd_client : `EfdClient`\n",
    "        EfdClient to query the efd.\n",
    "    obsenv_client: `EfdClient`\n",
    "        EfdClient to query the obsenv (different database).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    script_status : `pd.DataFrame`\n",
    "        DataFrame containing script description, configuration, timing information and states.\n",
    "\n",
    "\n",
    "    Note\n",
    "    ----\n",
    "    The index of the returned dataframe is chosen from the timestamps recorded for the script. \n",
    "    In order to best place the script message inline with other events such as acquired images,\n",
    "    the time used is the `timestampRunStart` if available, `timestampConfigureEnd` next, and\n",
    "    then falls back to `timestampConfigureStart` or `timestampProcessStart` if those are also not\n",
    "    available.\n",
    "    \"\"\"\n",
    "\n",
    "    # The script_salIndex is ONLY unique during the time that a particular queue remains not OFFLINE\n",
    "    # However, each queue can go offline independently, so the time intervals that are required for each queue\n",
    "    # can be different, and requires inefficient querying of the lsst.sal.Script topics (which don't include \n",
    "    # the queue identification explicitly). Furthermore, the downtime is infrequent, so probably we'd\n",
    "    # most of the time prefer to do the efficient thing and query everything all at once. \n",
    "\n",
    "    # So first - see if that's possible.\n",
    "    topic = 'lsst.sal.ScriptQueue.logevent_summaryState'\n",
    "    fields = ['salIndex', 'summaryState']\n",
    "    # Were there breaks in this queue?\n",
    "    dd = await efd_client.select_time_series(topic, fields, t_start, t_end)\n",
    "    if len(dd) == 0:\n",
    "        offline_events = 0\n",
    "    else:\n",
    "        offline_state = CSCState.OFFLINE.value\n",
    "        offline_events = len(dd.query('summaryState == @offline_state'))\n",
    "    \n",
    "    if offline_events == 0:\n",
    "        print(f\"No OFFLINE events during time interval {t_start} to {t_end} for any queue.\")\n",
    "        # So then go ahead and just do a single big query.\n",
    "        script_stream = await get_script_stream(t_start, t_end, efd_client)\n",
    "        script_status = await get_script_state(t_start, t_end, None, efd_client)\n",
    "        script_status = pd.merge(script_stream, script_status, left_on='script_salIndex', right_index=True, suffixes=['', '_s'])\n",
    "    \n",
    "    else:\n",
    "        # The ScriptQueues can be started and stopped independently, so run needs to run per-scriptqueue, per-uptime\n",
    "        script_status = []\n",
    "        for queue in SalIndex:\n",
    "            topic = 'lsst.sal.ScriptQueue.logevent_summaryState'\n",
    "            fields = ['salIndex', 'summaryState']\n",
    "            # Were there breaks in this particular queue?\n",
    "            dd = await efd_client.select_time_series(topic, fields, t_start, t_end, index=queue)\n",
    "            if len(dd) == 0:\n",
    "                tstops = []\n",
    "                tintervals = [[t_start, t_end]]\n",
    "            else:\n",
    "                dd['state'] = dd.apply(apply_enum, args=['summaryState', CSCState], axis=1)\n",
    "                dd['state_time'] = Time(dd.index.values)\n",
    "            \n",
    "                tstops = dd.query('state == \"OFFLINE\"').state_time.values\n",
    "                if len(tstops) == 0:\n",
    "                    tintervals = [[t_start, t_end]]\n",
    "                if len(tstops) > 0:\n",
    "                    ts = tstops[0]\n",
    "                    ts_next = ts + TimeDelta(0.1 * u.second)\n",
    "                    ts_next = Time(ts_next)\n",
    "                    tintervals = [[t_start, ts]]    \n",
    "                    for ts in tstops[1:]:\n",
    "                        tintervals.append([ts_next, ts])\n",
    "                        ts_next = ts + TimeDelta(0.1 * u.second)\n",
    "                    tintervals.append([ts_next, t_end])\n",
    "            if len(tstops) == 0:\n",
    "                print(f\"For {queue.name}, found 0 ScriptQueue OFFLINE events in the time period  {t_start} to {t_end}.\")\n",
    "            else:\n",
    "                print(f\"For {queue.name}, found {len(tstops)} ScriptQueue restarts in the time period {t_start} to {t_end}, so will query in {len(tstops)+1} chunks\")\n",
    "                print(f\"OFFLINE event at @ {[t.utc.iso for t in tstops]}\")\n",
    "            \n",
    "            # Do the script queue queries for each time interval in this queue\n",
    "            for tinterval in tintervals:\n",
    "                script_stream_t = await get_script_stream(tinterval[0], tinterval[1], efd_client)    \n",
    "                script_status_t = await get_script_state(tinterval[0], tinterval[1], queue, efd_client)\n",
    "                # Merge with script_stream so we get better descriptions and configuration information\n",
    "                if len(script_status_t) == 0 or len(script_stream_t) == 0:\n",
    "                    dd = []\n",
    "                else:\n",
    "                    dd = pd.merge(script_stream_t, script_status_t, left_on='script_salIndex', right_index=True, suffixes=['', '_s'])\n",
    "                    script_status.append(dd)\n",
    "                print(f\"Found {len(dd)} script-status messages during {[e.iso for e in tinterval]} for {queue.name}\")\n",
    "        # Convert to a single dataframe\n",
    "        script_status = pd.concat(script_status)\n",
    "    \n",
    "    print(f\"Found {len(script_status)} script status messages\")\n",
    "    \n",
    "    # script_status columns: \n",
    "    # ['classname', 'description', 'script_salIndex', 'ScriptID', 'blockId',\n",
    "    # 'config', 'executionId', 'logLevel', 'pauseCheckpoint',\n",
    "    # 'stopCheckpoint', 'path', 'salIndex', 'finalScriptState', 'scriptState',\n",
    "    # 'processState', 'timestampProcessStart', 'timestampConfigureStart',\n",
    "    # 'timestampConfigureEnd', 'timestampRunStart', 'timestampProcessEnd'] \n",
    "    # columns used in final merged dataframe:\n",
    "    # ['time', 'name', 'description', 'config', 'script_salIndex', 'salIndex', \n",
    "    # 'finalStatus', 'timestampProcessStart', 'timestampConfigureEnd', 'timestampRunStart', 'timestampProcessEnd'] \n",
    "\n",
    "    def _find_best_script_time(x):\n",
    "        # Try run start first\n",
    "        best_time = x.timestampRunStart\n",
    "        if best_time == TIMESTAMP_ZERO:\n",
    "            best_time = x.timestampConfigureEnd\n",
    "        if best_time == TIMESTAMP_ZERO:\n",
    "            best_time = x.timestampConfigureStart\n",
    "        if best_time ==  TIMESTAMP_ZERO:\n",
    "            best_time = x.timestampProcessStart\n",
    "        return best_time    \n",
    "    # Create an index that will slot this into the proper place for runtime / image acquisition, etc\n",
    "    script_status.index = script_status.apply(_find_best_script_time, axis=1)\n",
    "    script_status.index = script_status.index.tz_localize(\"UTC\")\n",
    "    script_status.sort_index(inplace=True)\n",
    "    return script_status\n",
    "\n",
    "async def get_tracebacks(t_start: Time, t_end: Time, efd_client: EfdClient) -> pd.DataFrame:\n",
    "    \"\"\"Find tracebacks in lsst.sal.Script.logevent_logMessage.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    t_start : `astropy.Time`\n",
    "        The time to start searching for script events.\n",
    "    t_end : `astropy.Time`\n",
    "        The time at which to end searching for script events.\n",
    "    efd_client : `EfdClient`\n",
    "        EfdClient to query the efd.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tracebacks : `pd.DataFrame`\n",
    "        DataFrame containing tracebacks.\n",
    "    \"\"\"\n",
    "    # Add tracebacks for failed scripts -- these should just slot in right after FAILED scripts, and link with script_salIndex\n",
    "    topic = \"lsst.sal.Script.logevent_logMessage\"\n",
    "    fields = [\"message\", \"traceback\", \"salIndex\"]\n",
    "    traceback_messages = await efd_client.select_time_series(topic, fields, t_start, t_end)\n",
    "    traceback_messages.rename({'salIndex': 'script_salIndex'}, axis=1, inplace=True)\n",
    "    # Only keep the lines where the traceback wasn't empty.\n",
    "    traceback_messages.query('traceback != \"\"', inplace=True)\n",
    "    # Add salIndex of queue where the script was run\n",
    "    def queue_from_script_salindex(x):\n",
    "        return int(str(x.script_salIndex)[0])\n",
    "    traceback_messages['salIndex'] = traceback_messages.apply(queue_from_script_salindex, axis=1)\n",
    "    def make_config_message(x):\n",
    "        return f\"Traceback for {x.script_salIndex}\"\n",
    "    traceback_messages['config'] = traceback_messages.apply(make_config_message, axis=1)\n",
    "    # Going to rename some of these columns here, just because scheduler configs and script queue already match nicely\n",
    "    traceback_messages.rename({'traceback': 'description', 'message': 'classname'}, axis=1, inplace=True)\n",
    "    traceback_messages['finalScriptState'] = 'Traceback'\n",
    "    traceback_messages['timestampProcessStart'] = traceback_messages.index.copy().tz_localize(None).astype('datetime64[ns]')\n",
    "    return traceback_messages\n",
    "\n",
    "\n",
    "# Scheduler dependency information\n",
    "async def get_scheduler_configs(t_start: Time, t_end: Time, efd_client: EfdClient, obsenv_client: EfdClient) -> pd.DataFrame:    \n",
    "    # First find the obsenv to find the version of ts_config_ocs\n",
    "    topic = 'lsst.obsenv.summary'\n",
    "    fields = ['summit_extras', 'summit_utils',  'ts_standardscripts', 'ts_externalscripts', 'ts_config_ocs']\n",
    "    # Query longer time period for obsenv, so we can be sure to know how scheduler enables\n",
    "    # t_start_local is already at the point where we have a scheduler enable event\n",
    "    # But sometimes the obsenv has been updated quite a long time before that even (with auxtel at least)\n",
    "    t_start_local = t_start\n",
    "    step_back = TimeDelta(1, format='jd')\n",
    "    obsenv = await obsenv_client.select_time_series(topic, fields, t_start_local - TimeDelta(1, format='jd'), t_end)\n",
    "    i = 0\n",
    "    while len(obsenv) == 0 and i < 90:\n",
    "        t_start_local = t_start_local - step_back\n",
    "        obsenv = await obsenv_client.select_time_series(topic, fields, t_start_local - TimeDelta(1, format='jd'), t_end)\n",
    "        i += 1\n",
    "    if len(obsenv) == 0:\n",
    "        warnings.warn(f\"Could not find obsenv values within previous {(i-1) * step_back.jd} days\")\n",
    "        # This shouldn't happen, but could before obsenv was implemented.\n",
    "        # We need something to fill in for work below.\n",
    "        bad_obsenv0 = [(t_start - step_back * 3).utc.datetime] + ['unknown' for f in fields]\n",
    "        bad_obsenv1 = [t_start.utc.datetime] + ['unknown' for f in fields]\n",
    "        obsenv = pd.DataFrame([bad_obsenv0, bad_obsenv1], columns=['time'] + fields)\n",
    "        obsenv.set_index('time', inplace=True)\n",
    "        obsenv.index = obsenv.index.tz_localize(\"UTC\")\n",
    "    elif i > 1:\n",
    "        # If we looped backwards additional days to find previous obsenv, just use the last value.\n",
    "        obsenv = obsenv.iloc[:1]\n",
    "        \n",
    "    check = np.all((obsenv[fields][1:].values == obsenv[fields][:-1].values), axis=1)\n",
    "    classname = np.where(check, \"Obsenv Check\", \"Obsenv Update\")\n",
    "    obsenv['classname'] = np.concatenate([np.array(['Obsenv']), classname])\n",
    "    obsenv['description'] = (\"ts_config_ocs: \" + obsenv['ts_config_ocs'])\n",
    "    obsenv['config'] = (\"ts_standardscripts: \" + obsenv['ts_standardscripts'] + \n",
    "                        \"; ts_externalscripts: \" + obsenv['ts_externalscripts'] + \n",
    "                        \"; summit_utils: \" + obsenv['summit_utils'] + \n",
    "                        \"; summit_extras: \" + obsenv['summit_extras'])\n",
    "    # The obsenv is shared across all scriptqueues. The salIndex has to apply to all.\n",
    "    obsenv['salIndex'] = 0\n",
    "    obsenv['script_salIndex'] = -1\n",
    "\n",
    "    # Scheduler dependency information - updated independently of obsenv.\n",
    "    topic = 'lsst.sal.Scheduler.logevent_dependenciesVersions'\n",
    "    fields = await efd_client.get_fields(topic)\n",
    "    fields = [f for f in fields if \"private\" not in f]\n",
    "    # Sometimes the scheduler hasn't been set up, if it's a limited timespan.\n",
    "    t_start_local = t_start\n",
    "    deps = await efd_client.select_time_series(topic, fields, t_start_local, t_end)    \n",
    "    i = 0\n",
    "    while len(deps) == 0 and i < 90:         \n",
    "        t_start_local = t_start_local - step_back\n",
    "        deps = await efd_client.select_time_series(topic, fields, t_start_local, t_end)   \n",
    "        i += 1\n",
    "    if len(deps) == 0:\n",
    "        warnings.warn(f\"Could not find scheduler config within previous {(i-1) * step_back.jd} days\")\n",
    "        \n",
    "    if i > 1:\n",
    "        deps = deps.iloc[:1]\n",
    "    \n",
    "    # Reconfigure output to fit into script_status fields \n",
    "    deps['classname'] = \"Scheduler dependencies\"\n",
    "    deps['description'] = deps['scheduler'] + ' ' + deps['seeingModel']\n",
    "    models = [c for c in deps.columns if 'observatory' in c or 'Model' in c]\n",
    "    def build_dep_string(x, models): \n",
    "        dep_string = ''\n",
    "        for m in models:\n",
    "            dep_string += f\"{m}: {x[m]}, \"\n",
    "        dep_string = dep_string[:-2]\n",
    "        return dep_string\n",
    "    deps['config'] = deps.apply(build_dep_string, args=[models], axis=1)\n",
    "    deps['script_salIndex'] = -1\n",
    "    \n",
    "    # The configurationApplied should happen with every scheduler update\n",
    "    topic = 'lsst.sal.Scheduler.logevent_configurationApplied'\n",
    "    fields = await efd_client.get_fields(topic)\n",
    "    fields = [f for f in fields if \"private\" not in f]\n",
    "    con = await efd_client.select_time_series(topic, fields, t_start_local, t_end)\n",
    "    con['classname'] = \"Scheduler configuration\"\n",
    "    # Build description from schemaVersion (just in case) and ts_config_ocs \n",
    "    ts_config_ocs_in_place = []\n",
    "    for time in con.index:\n",
    "        prev_obsenv = obsenv.query('index < @time')\n",
    "        if len(prev_obsenv) == 0:\n",
    "            ts_config_ocs_in_place.append('Unknown')\n",
    "        else:\n",
    "            ts_config_ocs_in_place.append(prev_obsenv.iloc[-1]['ts_config_ocs'])\n",
    "    con['ts_config_ocs'] = ts_config_ocs_in_place\n",
    "    con['description'] = 'ts_config_ocs ' + con['ts_config_ocs'] + ' ' + con['schemaVersion']\n",
    "    con.rename({'configurations': 'config'}, axis=1, inplace=True)\n",
    "    con['script_salIndex'] = -1\n",
    "\n",
    "    # Combine results\n",
    "    dd =  pd.concat([deps, con, obsenv])\n",
    "    # Trim back results to t_start, keeping last previous update information\n",
    "    # Trim obsenv back to range for other values\n",
    "    # But keep last entry so we have easy record \n",
    "    tt = pd.to_datetime(t_start.utc.datetime).tz_localize(\"UTC\")\n",
    "    # Keep last scheduler configuration update\n",
    "    old_dd_sched = dd.query('index < @tt and classname == \"Scheduler configuration\"')[-1:]\n",
    "    old_dd_deps = dd.query('index < @tt and classname == \"Scheduler dependencies\"')[-1:]\n",
    "    old_dd_obsenv = dd.query('index < @tt and classname.str.contains(\"Obsenv\")')[-1:]\n",
    "    dd = dd.query('index >= @tt')\n",
    "    sched_config = pd.concat([old_dd_sched, old_dd_obsenv, old_dd_deps, dd])\n",
    "\n",
    "    # Reformat\n",
    "    cols = ['classname', 'description', 'config', 'salIndex', 'script_salIndex']\n",
    "    drop_cols = [c for c in sched_config.columns if c not in cols]\n",
    "    sched_config.drop(drop_cols, axis=1, inplace=True)\n",
    "    sched_config.sort_index(inplace=True)\n",
    "    sched_config['timestampProcessStart'] = sched_config.index.copy().tz_localize(None).astype('datetime64[ns]')\n",
    "    sched_config['finalScriptState'] = \"Configuration\"\n",
    "    print(f\"Found {len(sched_config)} scheduler configuration records\")\n",
    "    return sched_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c215f3-9024-4339-8fde-d970b2764757",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_error_codes(t_start: Time, t_end: Time, efd_client: EfdClient) -> pd.DataFrame:\n",
    "    \"\"\"Get all messages from logevent_errorCode topics.\"\"\"\n",
    "    # Get error codes\n",
    "    topics = await efd_client.get_topics()\n",
    "    err_codes = [t for t in topics if 'errorCode' in t]\n",
    "    \n",
    "    errs = []\n",
    "    for topic in err_codes:\n",
    "        df = await efd_client.select_time_series(topic, ['errorCode', 'errorReport'], t_start, t_end)\n",
    "        if len(df) > 0:\n",
    "            df['topic'] = topic\n",
    "            errs += [df]\n",
    "    if len(errs) > 0:\n",
    "        errs = pd.concat(errs).sort_index()\n",
    "        def strip_csc(x):\n",
    "            return x.topic.replace(\"lsst.sal\", \"\").replace(\"logevent_errorCode\", \"\").replace(\".\", \"\") + \"CSC error\"\n",
    "        errs['component'] = errs.apply(strip_csc, axis=1)\n",
    "        # Rename some columns to match narrative log columns\n",
    "        errs.rename({'errorCode': 'error_code', 'errorReport': 'message_text', 'topic': 'origin'}, axis=1, inplace=True)\n",
    "        # Add a salindex so we can color-code based on this as a \"source\"\n",
    "        errs['salIndex'] = 4\n",
    "        errs['finalStatus'] = \"ERR\"\n",
    "        errs['timestampProcessStart'] = errs.index.values.copy()\n",
    "    else:\n",
    "        # Make an empty dataframe.\n",
    "        errs = pd.DataFrame([], columns=['component', 'error_code', 'message_text', 'origin', 'salIndex', 'finalStatus', 'timestampProcessStart'])\n",
    "    \n",
    "    print(f\"Found {len(errs)} error messages\")\n",
    "    return errs\n",
    "\n",
    "async def get_watcher_alarms(t_start: Time, t_end: Time, efd_client: EfdClient) -> pd.DataFrame:\n",
    "    \"\"\"Get and consolidate watcher alarms from lsst.sal.Watcher.logevent_alarm topic.\"\"\"\n",
    "    topic = 'lsst.sal.Watcher.logevent_alarm'\n",
    "    fields = await efd_client.get_fields(topic)\n",
    "    fields = [f for f in fields if ('private' not in f) and (f != 'name') and (f != 'duration')]\n",
    "    watcher_messages = await efd_client.select_time_series(topic, fields, t_start, t_end)\n",
    "    # Convert severity to readable string.\n",
    "    watcher_messages['severity'] = watcher_messages.apply(apply_enum, args=('severity', AlarmSeverity), axis=1)\n",
    "    # Convert times for readability.\n",
    "    for col in [c for c in watcher_messages.columns if 'timestamp' in c]:\n",
    "        watcher_messages[col] = Time(watcher_messages[col], format='unix_tai').utc.datetime\n",
    "    # Join on reason to consolidate messages, then join on timestampAcknowledged and timestampSeverityOldest\n",
    "    watcher_messages = watcher_messages.groupby(['reason', 'timestampAcknowledged']).first() \n",
    "    watcher_messages.reset_index(drop=False, inplace=True)\n",
    "    # Join watcher messages based on timestampSeverityOldest too, maybe\n",
    "    watcher_messages = watcher_messages.groupby('timestampSeverityOldest').first()\n",
    "    watcher_messages.reset_index(drop=False, inplace=True)\n",
    "    watcher_messages.index = watcher_messages['timestampSeverityOldest'].copy()\n",
    "    watcher_messages.index.names = [None]\n",
    "    watcher_messages.index = watcher_messages.index.tz_localize(\"UTC\")\n",
    "    # And since the timestampSeverityOldest can be different while the future \n",
    "    # Rename some columns for merge with errors \n",
    "    watcher_messages.rename({'reason': 'message_text', 'escalateTo': 'component',  'acknowledgedBy': 'origin', 'severity': 'error_code'}, axis=1, inplace=True)\n",
    "    watcher_messages['salIndex'] = 4\n",
    "    watcher_messages['error_code'] = 0\n",
    "    watcher_messages['finalStatus'] = \"ALARM\"\n",
    "    print(f\"Found {len(watcher_messages)} watcher messages\")\n",
    "    return watcher_messages.sort_index()\n",
    "\n",
    "\n",
    "def get_narrative_log(t_start: Time, t_end: Time, narrative_log_endpoint: str, auth: dict) -> pd.DataFrame:\n",
    "    \"\"\"Get the narrative log entries.\"\"\"    \n",
    "    log_limit = 50000\n",
    "    params = {\"is_human\" : \"either\",\n",
    "              \"is_valid\" : \"true\",\n",
    "              \"has_date_begin\" : True,\n",
    "              \"min_date_begin\" : t_start.to_datetime(),\n",
    "              \"max_date_begin\" : t_end.to_datetime(),\n",
    "              \"order_by\" : \"date_begin\",\n",
    "              \"limit\": log_limit, \n",
    "             }\n",
    "    messages = query_logging_services(narrative_log_endpoint, auth=auth, params=params)\n",
    "    # Modify narrative log content to match dataframes from errors and watcher topics better.\n",
    "    # Strip out repeated \\n\\n and \\r\\n characters for nicer printing in dataframe.\n",
    "    if len(messages) > 0:\n",
    "        def strip_rns(x):\n",
    "            return x.message_text.replace(\"\\r\\n\", \"\\n\").replace(\"\\n\\n\", \"\\n\").rstrip(\"\\n\")\n",
    "        def make_time(x, column):\n",
    "            return Time(x[column], format='isot', scale='tai').utc.datetime\n",
    "        def clarify_log(x):\n",
    "            if x.components is None:\n",
    "                component = \"Log\"\n",
    "            else:\n",
    "                component = \"Log \" + \" \".join(x.components)\n",
    "            return component\n",
    "        # Strip excessive \\r\\n and \\n\\n from messages\n",
    "        messages['message_text'] = messages.apply(strip_rns, axis=1)\n",
    "        # Add a time index - use date_added as this makes them align best at present\n",
    "        messages['time'] = messages.apply(make_time, args=[\"date_added\"], axis=1)\n",
    "        messages.set_index('time', inplace=True)\n",
    "        messages.index = messages.index.tz_localize(\"UTC\")\n",
    "        # Join the components and add \"Log\" explicitly\n",
    "        messages['component'] = messages.apply(clarify_log, axis=1)\n",
    "        # rename some columns to match error data\n",
    "        messages.rename({'time_lost_type': 'error_code', 'user_id': 'origin'}, axis=1, inplace=True)\n",
    "        # Add a salindex so we can color-code based on this as a \"source\"\n",
    "        messages['salIndex'] = 0\n",
    "        messages['error_code'] = 0\n",
    "        messages['finalStatus'] = \"Log\"\n",
    "        messages['timestampProcessStart'] = messages.apply(make_time, args=[\"date_begin\"], axis=1)\n",
    "        messages['timestampRunStart'] = messages.apply(make_time, args=[\"date_added\"], axis=1)\n",
    "        messages['timestampProcessEnd'] = messages.apply(make_time, args=[\"date_end\"], axis=1)\n",
    "    print(f\"Found {len(messages)} messages in the narrative log\")\n",
    "    if len(messages) == log_limit:\n",
    "        print(f\"Whoops, likely lost some log messages due to limit of {log_limit}.\")\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8458d9d-b7e8-44a3-a771-bd7aff30cb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_narrative_and_errs(t_start: Time, t_end: Time, efd_client: EfdClient, \n",
    "                                 narrative_log_endpoint: str | None, auth: dict | None, \n",
    "                                 include_watcher : bool = True) -> pd.DataFrame:\n",
    "    \"\"\"Get narrative, errorCode and (possibly) watcher alarms.\"\"\"\n",
    "\n",
    "    if narrative_log_endpoint is not None:\n",
    "        messages = get_narrative_log(t_start, t_end, narrative_log_endpoint, auth)\n",
    "    else:\n",
    "        messages = pd.DataFrame([])\n",
    "        \n",
    "    errs = await get_error_codes(t_start, t_end, efd_client)\n",
    "    if include_watcher:\n",
    "        watcher = await get_watcher_alarms(t_start,  t_end, efd_client)\n",
    "    else:\n",
    "        # Maybe we'll get some of the messages, for start of the night state, for now\n",
    "        # watcher = await get_watcher_alarms(t_start,  t_end, efd_client)\n",
    "        # watcher = watcher.query('message_text.str.len() > 100')\n",
    "        watcher = pd.DataFrame([])\n",
    "        print(f\"Kept {len(watcher)} watcher messages\")\n",
    "        \n",
    "    # Merge narrative log messages and error messages    \n",
    "    narrative_and_errs = pd.concat([errs, watcher, messages]).sort_index()\n",
    "    ncols = ['component', 'origin', 'message_text', 'error_code', 'salIndex', 'timestampSeverityOldest', 'timestampAcknowledged', 'timestampMaxSeverity']\n",
    "    return narrative_and_errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef4c636-e972-411b-bf49-363e8e86e680",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_exposure_info(t_start: Time, t_end: Time, efd_client: EfdClient, \n",
    "                            exposure_log_endpoint: str | None, auth: dict | None) -> pd.DataFrame:\n",
    "    \"\"\"Get exposure information from lsst.sal.CCCamera.logevent_endOfImageTelemetry\n",
    "    and join it with exposure log information. \n",
    "    \"\"\"\n",
    "    # Find exposure information - Simonyi Tel\n",
    "    topic = 'lsst.sal.CCCamera.logevent_endOfImageTelemetry' \n",
    "    fields = ['imageName', 'imageIndex', 'exposureTime', 'darkTime', 'measuredShutterOpenTime', \n",
    "              'additionalValues', 'timestampAcquisitionStart', 'timestampDateEnd', 'timestampDateObs']\n",
    "    image_acquisition_cc = await efd_client.select_time_series(topic, fields, t_start, t_end)\n",
    "    # If there were zero images in this timeperiod, just return now.\n",
    "    if len(image_acquisition_cc) > 0:\n",
    "        for col in [c for c in image_acquisition_cc.columns if c.startswith(\"timestamp\")]:\n",
    "            image_acquisition_cc[col] = Time(image_acquisition_cc[col], format='unix_tai').utc.datetime\n",
    "        image_acquisition_cc['salIndex'] = 5\n",
    "        image_acquisition_cc['script_salIndex'] = 0\n",
    "        image_acquisition_cc['finalStatus'] = \"Image Acquired\"\n",
    "        def make_config_col_for_image(x):\n",
    "            return f\"exp {x.exposureTime} // dark {x.darkTime} // open {x.measuredShutterOpenTime} \"\n",
    "        image_acquisition_cc['config'] = image_acquisition_cc.apply(make_config_col_for_image, axis=1)\n",
    "        image_acquisition_cc.index = image_acquisition_cc['timestampAcquisitionStart'].copy()\n",
    "        image_acquisition_cc.index = image_acquisition_cc.index.tz_localize(\"UTC\")\n",
    "        print(f\"Found {len(image_acquisition_cc)} image times for Simonyi\")\n",
    "        \n",
    "    # Find exposure information - Aux Tel\n",
    "    topic = 'lsst.sal.ATCamera.logevent_endOfImageTelemetry' \n",
    "    fields = ['imageName', 'imageIndex', 'exposureTime', 'darkTime', 'measuredShutterOpenTime', \n",
    "              'additionalValues', 'timestampAcquisitionStart', 'timestampDateEnd', 'timestampDateObs']\n",
    "    image_acquisition_at = await efd_client.select_time_series(topic, fields, t_start, t_end)\n",
    "    # If there were zero images in this timeperiod, just return now.\n",
    "    if len(image_acquisition_at) > 0:\n",
    "        for col in [c for c in image_acquisition_at.columns if c.startswith(\"timestamp\")]:\n",
    "            # Is it possible ATCamera is not using tai?\n",
    "            image_acquisition_at[col] = Time(image_acquisition_at[col], format='unix_tai').utc.datetime\n",
    "        image_acquisition_at['salIndex'] = 6\n",
    "        image_acquisition_at['script_salIndex'] = 0\n",
    "        image_acquisition_at['finalStatus'] = \"Image Acquired\"\n",
    "        def make_config_col_for_image(x):\n",
    "            return f\"exp {x.exposureTime} // dark {x.darkTime} // open {x.measuredShutterOpenTime} \"\n",
    "        image_acquisition_at['config'] = image_acquisition_at.apply(make_config_col_for_image, axis=1)\n",
    "        image_acquisition_at.index = image_acquisition_at['timestampAcquisitionStart'].copy()\n",
    "        image_acquisition_at.index = image_acquisition_at.index.tz_localize(\"UTC\")\n",
    "        print(f\"Found {len(image_acquisition_at)} image times for AuxTel\")\n",
    "\n",
    "    image_acquisition = pd.concat([image_acquisition_cc, image_acquisition_at])\n",
    "\n",
    "    # Now get exposure log information if exposure_log_endpoint defined.\n",
    "    if exposure_log_endpoint is not None:\n",
    "        log_limit = 50000\n",
    "        # A cheap conversion to dayobs int\n",
    "        min_dayobs_int = int(t_start.iso[0:10].replace('-', ''))\n",
    "        max_dayobs_int = int(t_end.iso[0:10].replace('-', ''))\n",
    "        params = {\"is_human\" : \"either\",\n",
    "                  \"is_valid\" : \"true\",\n",
    "                  \"min_day_obs\" : min_dayobs_int,\n",
    "                  \"max_day_obs\" : max_dayobs_int,\n",
    "                  \"limit\": log_limit, \n",
    "                 }\n",
    "        \n",
    "        exp_logs = query_logging_services(exposure_log_endpoint, auth=auth, params=params)\n",
    "        print(f\"Found {len(exp_logs)} messages in the exposure log\")\n",
    "        \n",
    "        # Modify exposure log and match with exposures to add time tag.\n",
    "        if len(exp_logs) > 0:\n",
    "            # Find a time to add the exposure logs into the records (next to the image).\n",
    "            exp = pd.merge(image_acquisition, exp_logs, how='right', left_on='imageName', right_on='obs_id')\n",
    "            # Set the time for the exposure log just slightly after the image start time\n",
    "            exp_log_image_time = exp['timestampAcquisitionStart'] + EPS_TIME\n",
    "            exp_logs['img_time'] = exp_log_image_time\n",
    "            exp_logs.set_index('img_time', inplace=True)\n",
    "            exp_logs.index = exp_logs.index.tz_localize(\"UTC\")\n",
    "            exp_logs['salIndex'] = 0\n",
    "            exp_logs['script_salIndex'] = 0\n",
    "            # Rename some columns in the exposure log so that we can consolidate them here\n",
    "            exp_logs.rename({'obs_id': 'imageName', 'user_id': 'config', 'message_text': 'additionalValues', 'exposure_flag': 'finalStatus'}, axis=1, inplace=True)\n",
    "            image_acquisition = pd.concat([image_acquisition, exp_logs]).sort_index()\n",
    "            print(\"Joined exposure and exposure log\")\n",
    "    return image_acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dcdf84-7983-4b00-bedc-b2ad15e1869f",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_consolidated_messages(t_start: Time, t_end: Time, include_watcher: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"Get consolidated messages from EFD ScriptQueue, errorCodes, CCCamera, exposure and narrative logs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    t_start : `astropy.Time`\n",
    "        Time of the start of the messages.\n",
    "    t_end : `astropy.Time`\n",
    "        Time of the end of the messages.\n",
    "    include_watcher : `bool`\n",
    "        Include messages from Watcher.logevent_alarms?\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    efd_and_messages : `pd.DataFrame`\n",
    "    \"\"\"\n",
    "    endpoints = get_clients()\n",
    "    print(endpoints['string'])\n",
    "    \n",
    "    # Now rename columns so we can put these all into the same dataframe\n",
    "    # goal columns : \n",
    "    cols = ['time', 'name', 'description', 'config', 'script_salIndex', 'salIndex', 'finalStatus', 'timestampProcessStart', 'timestampConfigureEnd', 'timestampRunStart', 'timestampProcessEnd'] \n",
    "    \n",
    "    # columns from scripts\n",
    "    script_status = await get_script_status(t_start, t_end, endpoints['efd'])\n",
    "    # script_cols = ['classname', 'description', 'config', 'script_salIndex', 'salIndex', 'blockId', 'finalScriptState', 'scriptState', 'timestampProcessStart', 'timestampConfigureEnd', 'timestampRunStart', 'timestampProcessEnd']\n",
    "    tracebacks = await get_tracebacks(t_start, t_end, endpoints['efd'])\n",
    "    scheduler_configs = await get_scheduler_configs(t_start, t_end, endpoints['efd'], endpoints['obsenv'])\n",
    "    script_status = pd.concat([scheduler_configs, script_status, tracebacks])\n",
    "    script_status.rename({'classname': 'name', 'finalScriptState': 'finalStatus'}, axis=1, inplace=True)\n",
    "    \n",
    "    # columns from narrative and errors\n",
    "    narrative_and_errs = await get_narrative_and_errs(t_start, t_end, endpoints['efd'], endpoints['api_base'] + endpoints['narrative_log'], endpoints['auth'], include_watcher=include_watcher)\n",
    "    # narrative_cols = ['component', 'origin', 'message_text', 'error_code', 'salIndex']\n",
    "    # if include_watcher:\n",
    "    #     narrative_cols = narrative_cols + ['timestampSeverityOldest', 'timestampAcknowledged', 'timestampMaxSeverity']\n",
    "    narrative_and_errs.rename({'component': 'name', 'origin': 'config', 'message_text': 'description', 'error_code': 'script_salIndex'}, axis=1, inplace=True)\n",
    "    if 'timestampSeverityOldest' in narrative_and_errs.columns:\n",
    "        narrative_and_errs.rename({'timestampSeverityOldest': 'timestampProcessStart', 'timestampAcknowledged': 'timestampConfigureEnd', 'timestampMaxSeverity': 'timestampRunStart'}, axis=1, inplace=True)\n",
    "    \n",
    "    # columns from images_and_logs\n",
    "    image_and_logs = await get_exposure_info(t_start, t_end, endpoints['efd'], endpoints['api_base'] + endpoints['exposure_log'], endpoints['auth'])\n",
    "    # image_cols = ['imageName', 'additionalValues', 'config', 'finalStatus', 'script_salIndex', 'salIndex', 'timestampAcquisitionStart', 'timestampDateObs', 'timestampDateEnd']\n",
    "    image_and_logs.rename({'imageName': 'name', 'additionalValues' : 'description', \n",
    "                           'timestampAcquisitionStart': 'timestampProcessStart', 'timestampDateObs': 'timestampRunStart', 'timestampDateEnd': 'timestampProcessEnd'}, axis=1, inplace=True) \n",
    "\n",
    "    efd_and_messages = pd.concat([script_status, narrative_and_errs, image_and_logs]).sort_index()\n",
    "    # Wrap description, which can may have long zero-space messages in the errors\n",
    "    efd_and_messages['description'] = efd_and_messages['description'].str.wrap(100)\n",
    "\n",
    "    # Add some big labels which could be used to indicate foldups\n",
    "    # The blocks can be complicated - a single BLOCK can actually trigger multiple AddBlock commands (?)\n",
    "    # So go back and check command_addBlock directly.\n",
    "    topic = 'lsst.sal.Scheduler.command_addBlock'\n",
    "    block_names = await endpoints['efd'].select_time_series(topic, ['id'], t_start, t_end, index=None)\n",
    "    # Find the FBS setup and starts\n",
    "    fbs_resume_times = efd_and_messages.query('name == \"MTSchedulerResume\"')\n",
    "    scheduler_configs = efd_and_messages.query('name == \"Scheduler configuration\"')\n",
    "    def find_fbs_yaml(row, scheduler_configs):\n",
    "        earlier_configs = scheduler_configs.query('index < @row.name')\n",
    "        best_config = earlier_configs.iloc[-1].config\n",
    "        return best_config.split(',')[-1]\n",
    "    sched_yamls = fbs_resume_times.apply(find_fbs_yaml, args=[scheduler_configs], axis=1)\n",
    "    sched_yamls = pd.DataFrame(sched_yamls, columns=['id'])\n",
    "    if len(block_names) > 0 and len(sched_yamls) > 0:\n",
    "        foldups = pd.concat([block_names, sched_yamls])\n",
    "    elif len(block_names) == 0:\n",
    "        foldups = sched_yamls\n",
    "    else:\n",
    "        foldups = block_names\n",
    "\n",
    "    if len(foldups) > 0:\n",
    "        # If we actually have some addBlock or resumeScheduler events, add those. \n",
    "        # Note that we could have images and events -- running from scripts. \n",
    "        # .. but I don't know how to track these.\n",
    "        foldups = foldups.sort_index()\n",
    "        foldups.rename({'id': 'name'}, axis=1, inplace=True)\n",
    "        foldups['salIndex'] = 0\n",
    "        foldups['script_salIndex'] = -1\n",
    "        foldups['finalStatus'] = 'Job Change'\n",
    "        foldups['config'] = ''\n",
    "        foldups['description'] = 'New BLOCK or FBS configuration'\n",
    "        foldups['timestampProcessStart'] = foldups.index.copy()\n",
    "        foldups['timestampProcessEnd'] = np.concatenate([foldups.index[1:].copy(), np.array([efd_and_messages.index[-1]])])\n",
    "        efd_and_messages = pd.concat([efd_and_messages, foldups]).sort_index()\n",
    "\n",
    "\n",
    "    # use an integer index, which makes it easier to pull up values plus avoids occasional failures of time uniqueness\n",
    "    efd_and_messages.reset_index(drop=False, inplace=True)\n",
    "    efd_and_messages.rename({'index': 'time'}, axis=1, inplace=True)\n",
    "\n",
    "    print(f\"Total combined messages {len(efd_and_messages)}\")\n",
    "\n",
    "    \n",
    "    return efd_and_messages, cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2261ca-9577-4b2e-8805-9cfa11dce825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define name and colours from salIndex    \n",
    "def get_name_and_color_from_salindex(sal_index, unknown_color='#f9f9f9'):\n",
    "    # Colors from https://medialab.github.io/iwanthue/\n",
    "    return {\n",
    "        5: ('Simonyi Exposure', '#b6ecf5'),\n",
    "        6: ('Auxtel Exposure', '#d8f1f5'),\n",
    "        0: ('Narrative log', '#cf7ddc'),\n",
    "        1: ('MTQueue', '#b4c546'),\n",
    "        2: ('ATQueue', '#bab980'),\n",
    "        3: ('OCSQueue', '#b2baad'),\n",
    "        4: ('EFD error', '#9cb5d5'),\n",
    "    }.get(sal_index, (\"??\", unknown_color))\n",
    "    \n",
    "# Add a custom formatter to handle YAML-like strings with dynamic background colors\n",
    "def format_config_as_yaml_with_colors(row):\n",
    "    config_value = row['config']\n",
    "    sal_index = row['salIndex']\n",
    "    script_salindex = row['script_salIndex']\n",
    "    \n",
    "    # Define background colors based on salIndex\n",
    "    background_color = get_name_and_color_from_salindex(sal_index)[1]\n",
    "\n",
    "    might_be_yaml = (script_salindex > 0) and (sal_index in [1, 2, 3])\n",
    "    might_be_yaml = might_be_yaml and isinstance(config_value, str) and len(config_value) > 0\n",
    "    might_be_yaml = might_be_yaml and not config_value.startswith('Traceback')\n",
    "\n",
    "    #if script_salindex > 0 and sal_index in [1,2,3] and isinstance(config_value, str) and len(config_value) > 0:\n",
    "    if might_be_yaml:\n",
    "        try:\n",
    "            # Parse the YAML-like string\n",
    "            parsed_yaml = yaml.safe_load(config_value)\n",
    "            # Format back to YAML with proper indentation\n",
    "            formatted_yaml = yaml.dump(parsed_yaml, default_flow_style=False)\n",
    "            return (\n",
    "                f\"<pre style='background: {background_color}; padding: 10px; border: 1px solid #ddd; margin: 0;'>\"\n",
    "                f\"{formatted_yaml}</pre>\"\n",
    "            )\n",
    "        except yaml.YAMLError:\n",
    "            # If parsing fails, return as plain text in a styled <pre> block\n",
    "            return (\n",
    "                f\"<pre style='background: {background_color}; padding: 10px; border: 1px solid #ddd; margin: 0;'>\"\n",
    "                f\"{config_value}</pre>\"\n",
    "            )\n",
    "    elif config_value.startswith('Traceback'):\n",
    "        return f\"<pre style='background: {background_color}'>{config_value}</pre>\"\n",
    "    else:\n",
    "        return config_value  # Return as-is if salIndex is 0 or invalid type\n",
    "\n",
    "def format_tracebacks(row):\n",
    "    return html.escape(row.description)\n",
    "\n",
    "    \n",
    "def pretty_print_messages(efd_and_messages: pd.DataFrame, cols: list, time_order: str,\n",
    "                         show_salIndex: list[int] = [0, 1, 2, 3, 4, 5]) -> None:\n",
    "\n",
    "    keep = np.zeros(len(efd_and_messages), dtype=bool)\n",
    "    for si in show_salIndex:\n",
    "        keep |= efd_and_messages.salIndex == si\n",
    "    efd_and_messages = efd_and_messages[keep]\n",
    "    \n",
    "    def highlight_salindex(s):\n",
    "        return [f'background-color: {get_name_and_color_from_salindex(s.salIndex)[1]}'] * len(s)\n",
    "    msg = [\"Color coding by \"]\n",
    "    for i in np.sort(efd_and_messages.salIndex.unique()):\n",
    "        what, color = get_name_and_color_from_salindex(i)\n",
    "        msg.append(f\" <font style='background-color: {color[0:]};'>{what}</font> \")\n",
    "    display(HTML(\" \".join(msg)))\n",
    "    \n",
    "    if time_order == \"newest first\": \n",
    "        efd_and_messages = efd_and_messages[::-1]\n",
    "    # Apply yaml-like formatting conditionally\n",
    "    efd_and_messages['config'] = efd_and_messages.apply(format_config_as_yaml_with_colors, axis=1)\n",
    "    efd_and_messages['description'] = efd_and_messages.apply(format_tracebacks, axis=1)\n",
    "    # Adjust the display call to include the formatted column\n",
    "    styled_table = (\n",
    "        efd_and_messages[cols]\n",
    "        .style.apply(highlight_salindex, axis=1)  # Preserve color formatting for other columns\n",
    "        .set_table_styles([dict(selector='th', props=[('text-align', 'left')])])\n",
    "        .set_properties(**{'text-align': 'left'})\n",
    "    )\n",
    "    \n",
    "    # Render with HTML\n",
    "    display(HTML(styled_table.format().to_html()))\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a81bc59-c967-4652-855a-905ec40910df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a range of times to search, based on dayobs\n",
    "if day_obs_min.lower() == \"today\":\n",
    "    # Shift the 12hour offset following the definition of day_obs in https://sitcomtn-032.lsst.io/    \n",
    "    # Drop the hours, minutes, seconds to get the ISO formatted day_obs\n",
    "    day_obs_min = Time(np.floor(Time.now().mjd - 0.5), format='mjd', scale='utc').iso[0:10]\n",
    "\n",
    "if day_obs_min.lower() == \"yesterday\":\n",
    "    # Shift the 12hour offset following the definition of day_obs in https://sitcomtn-032.lsst.io/\n",
    "    # Drop the hours, minutes, seconds to get the ISO fromatted day_obs\n",
    "    day_obs_min = (Time(np.floor(Time.now().mjd - 0.5), format='mjd', scale='utc') - TimeDelta(1, format='jd')).iso[0:10]\n",
    "\n",
    "# Set a range of times to search, based on dayobs\n",
    "if day_obs_max.lower() == \"today\":\n",
    "    # Shift the 12hour offset following the definition of day_obs in https://sitcomtn-032.lsst.io/    \n",
    "    # Drop the hours, minutes, seconds to get the ISO formatted day_obs\n",
    "    day_obs_max = Time(np.floor(Time.now().mjd - 0.5), format='mjd', scale='utc').iso[0:10]\n",
    "\n",
    "if day_obs_max.lower() == \"yesterday\":\n",
    "    # Shift the 12hour offset following the definition of day_obs in https://sitcomtn-032.lsst.io/\n",
    "    # Drop the hours, minutes, seconds to get the ISO fromatted day_obs\n",
    "    day_obs_max = (Time(np.floor(Time.now().mjd - 0.5), format='mjd', scale='utc') - TimeDelta(1, format='jd')).iso[0:10]\n",
    "\n",
    "try:\n",
    "    t_start = Time(f\"{day_obs_min}T12:00:00\", format='isot', scale='utc')\n",
    "except ValueError:\n",
    "    print(f\"Is day_obs_min the right format? {day_obs_min} should be YYYY-MM-DD\")\n",
    "    t_start = None\n",
    "try:\n",
    "    t_end = Time(f\"{day_obs_max}T12:00:00\", format='isot', scale='utc') + TimeDelta(1, format='jd')\n",
    "except ValueError:\n",
    "    print(f\"Is day_obs_max the right format? {day_obs_max} should be YYYY-MM-DD\")\n",
    "    t_start = None\n",
    "\n",
    "if t_start is None or t_end is None:\n",
    "    print(\"Did not get valid inputs for time period.\")\n",
    "    \n",
    "\n",
    "print(f\"Querying for messages from {t_start.iso} to {t_end.iso}\")\n",
    "print(f\"Notebook executed at {Time.now().utc.iso}\")\n",
    "efd_and_messages, cols = await get_consolidated_messages(t_start, t_end)\n",
    "\n",
    "# Could add these to parameters\n",
    "save_log = False\n",
    "make_link = False\n",
    "\n",
    "if save_log:\n",
    "    log_filename = f\"log_{day_obs_min}_{day_obs_max}.h5\"\n",
    "    # We will always get a performance warning here, because the dataframe includes string objects\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        efd_and_messages[cols].to_hdf(log_filename, key='messages')\n",
    "        print(f\"Wrote to {log_filename}\")\n",
    "if make_link:\n",
    "    import base64\n",
    "    html_table = efd_and_messages[cols].to_xml(index=False)\n",
    "    b64 = base64.b64encode(html_table.encode())\n",
    "    payload = b64.decode()\n",
    "    log_xml =  f\"log_{day_obs_min}_{day_obs_max}.xml\"\n",
    "    html_link = f'<a download=\"{log_xml}\" href=\"data:text/csv;base64,{payload}\" target=\"_blank\">Download XML table of log messages</a>'\n",
    "    display(HTML(html_link))\n",
    "    print(\" read download with pandas.read_xml, convert times using .astype('datetime64[ns]')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d623216-2776-4b81-9a6e-6b1690cebb4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if isinstance(show_salIndex, str) and show_salIndex.lower() == 'all':\n",
    "    show_salIndex = efd_and_messages.salIndex.unique()\n",
    "# Ok, otherwise we have to do some parsing .. we get a string but need list of ints.\n",
    "showsal = []\n",
    "for i in show_salIndex:\n",
    "    try:\n",
    "        showsal.append(int(i))\n",
    "    except ValueError:\n",
    "        # Wasn't an integer, pass\n",
    "        # easier when no negative int salIndexes.\n",
    "        pass\n",
    "show_salIndex = showsal\n",
    "\n",
    "pretty_print_messages(efd_and_messages, cols, time_order, show_salIndex=show_salIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a1f3e3-6342-45c5-a7ed-52a2842fa8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import asyncio\n",
    "from astropy.time import Time, TimeDelta\n",
    "\n",
    "# --- Define your time range ---\n",
    "# (Adjust day_obs_min and day_obs_max as needed.)\n",
    "day_obs_min = \"2024-11-06\"\n",
    "day_obs_max = \"2024-11-07\"\n",
    "#day_obs_max = \"2024-11-18\"\n",
    "\n",
    "try:\n",
    "    t_start = Time(f\"{day_obs_min}T12:00:00\", format='isot', scale='utc')\n",
    "except ValueError:\n",
    "    raise ValueError(f\"day_obs_min should be in YYYY-MM-DD format: got {day_obs_min}\")\n",
    "try:\n",
    "    t_end = Time(f\"{day_obs_max}T12:00:00\", format='isot', scale='utc') + TimeDelta(1, format='jd')\n",
    "except ValueError:\n",
    "    raise ValueError(f\"day_obs_max should be in YYYY-MM-DD format: got {day_obs_max}\")\n",
    "\n",
    "print(f\"Querying for messages from {t_start.iso} to {t_end.iso}\")\n",
    "\n",
    "# --- Get the consolidated messages ---\n",
    "# (This call is asynchronous. Adjust if you are using an async-enabled notebook.)\n",
    "efd_and_messages, cols = await get_consolidated_messages(t_start, t_end)\n",
    "\n",
    "# --- Map salIndex values to group names ---\n",
    "salindex_mapping = {\n",
    "    5: 'Simonyi Exposure',\n",
    "    6: 'Auxtel Exposure',\n",
    "    0: 'Narrative log',\n",
    "    1: 'MTQueue',\n",
    "    2: 'ATQueue',\n",
    "    3: 'OCSQueue',\n",
    "    4: 'EFD error'\n",
    "}\n",
    "\n",
    "# --- Create a dictionary of DataFrames, one per group ---\n",
    "groups = {}\n",
    "for sal_index, group_name in salindex_mapping.items():\n",
    "    groups[group_name] = efd_and_messages[efd_and_messages['salIndex'] == sal_index]\n",
    "\n",
    "# --- (Optional) Print summary counts for each group ---\n",
    "print(\"Message counts by group:\")\n",
    "for group_name, df in groups.items():\n",
    "    print(f\"  {group_name}: {len(df)} messages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aefddf7-7ae5-4c90-b88b-1092cf1a7d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now you can work with each group individually.\n",
    "simonyi_df    = groups['Simonyi Exposure']\n",
    "auxtel_df     = groups['Auxtel Exposure']\n",
    "narrative_df  = groups['Narrative log']\n",
    "mtqueue_df    = groups['MTQueue']\n",
    "atqueue_df    = groups['ATQueue']\n",
    "ocsqueue_df   = groups['OCSQueue']\n",
    "efd_error_df  = groups['EFD error']\n",
    "\n",
    "# For example, to display the first few rows of each DataFrame:\n",
    "print(\"Simonyi Exposure:\")\n",
    "display(simonyi_df.head())\n",
    "\n",
    "print(\"Auxtel Exposure:\")\n",
    "display(auxtel_df.head())\n",
    "\n",
    "print(\"Narrative log:\")\n",
    "display(narrative_df.head())\n",
    "\n",
    "print(\"MTQueue:\")\n",
    "display(mtqueue_df.head())\n",
    "\n",
    "print(\"ATQueue:\")\n",
    "display(atqueue_df.head())\n",
    "\n",
    "print(\"OCSQueue:\")\n",
    "display(ocsqueue_df.head())\n",
    "\n",
    "print(\"EFD error:\")\n",
    "display(efd_error_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1fa92a-a5d7-4429-87e5-74e572d01526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import asyncio\n",
    "from astropy.time import Time, TimeDelta\n",
    "\n",
    "# --- Define tu rango de tiempo ---\n",
    "day_obs_min = \"2024-11-24\"\n",
    "day_obs_max = \"2024-11-24\"\n",
    "\n",
    "try:\n",
    "    t_start = Time(f\"{day_obs_min}T12:00:00\", format='isot', scale='utc')\n",
    "except ValueError:\n",
    "    raise ValueError(f\"day_obs_min should be in YYYY-MM-DD format: got {day_obs_min}\")\n",
    "try:\n",
    "    t_end = Time(f\"{day_obs_max}T12:00:00\", format='isot', scale='utc') + TimeDelta(1, format='jd')\n",
    "except ValueError:\n",
    "    raise ValueError(f\"day_obs_max should be in YYYY-MM-DD format: got {day_obs_max}\")\n",
    "\n",
    "print(f\"Querying for messages from {t_start.iso} to {t_end.iso}\")\n",
    "\n",
    "# --- Obtener los mensajes consolidados ---\n",
    "# (Esta llamada es asíncrona; ajústala según tu entorno)\n",
    "efd_and_messages, cols = await get_consolidated_messages(t_start, t_end)\n",
    "\n",
    "# --- Mapeo de salIndex a nombres de grupo ---\n",
    "salindex_mapping = {\n",
    "    5: 'Simonyi Exposure',\n",
    "    6: 'Auxtel Exposure',\n",
    "    0: 'Narrative log',\n",
    "    1: 'MTQueue',\n",
    "    2: 'ATQueue',\n",
    "    3: 'OCSQueue',\n",
    "    4: 'EFD error'\n",
    "}\n",
    "\n",
    "# --- Crear un diccionario de DataFrames, uno por grupo ---\n",
    "groups = {}\n",
    "for sal_index, group_name in salindex_mapping.items():\n",
    "    groups[group_name] = efd_and_messages[efd_and_messages['salIndex'] == sal_index]\n",
    "\n",
    "# --- (Opcional) Mostrar conteos por grupo ---\n",
    "print(\"Message counts by group:\")\n",
    "for group_name, df in groups.items():\n",
    "    print(f\"  {group_name}: {len(df)} messages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67228c3a-2ce3-48b7-ba00-657c1028b597",
   "metadata": {},
   "source": [
    "# AstroChat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd8892c-aadd-4ad2-be7b-abd4a84b9217",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_custom = mtqueue_df[['config', 'name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d2d95e-70bd-4651-81e2-e5201ea6d5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lsst.summit.extras.astrochat import set_api_key, AstroChat\n",
    "%matplotlib inline\n",
    "\n",
    "day_obs = 20241124\n",
    "set_api_key()\n",
    "chat = AstroChat(data=df_custom, day_obs=day_obs, verbosity='ALL', export=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f9a3ae-93fb-4fa5-9563-07ed7d380f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat.list_demos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc04b316-9d84-4bcc-8ae9-1bdc32270b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat.run('make a summary of the data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e4c17e-4dcb-4800-a382-4600ab656659",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSST",
   "language": "python",
   "name": "lsst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
