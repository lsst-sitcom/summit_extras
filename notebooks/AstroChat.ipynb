{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EXAMPLE 1: RAG ===\n",
      "Proposed Plan: 1. Access MAST through their website: https://mast.stsci.edu/\n",
      "2. Use the search bar to input keywords related to spectra observations, such as \"spectra,\" \"emission lines,\" or specific objects of interest.\n",
      "3. Filter results by selecting appropriate filters like wavelength range, object type, and instrument used for observation.\n",
      "4. Review the list of available datasets and select those that match your criteria.\n",
      "5. Download the data files for further analysis.\n",
      "Plan was not approved. Adjusting...\n",
      "[rag_worker] Executing: Find observation data about spectra.\n",
      "Final RAG Result: Tool not found: 'web_browser'.\n",
      "\n",
      "=== EXAMPLE 2: Python Code Execution ===\n",
      "Proposed Plan: 1. Define the mass, height, and acceleration due to gravity.\n",
      "2. Use the potential energy formula PE = mgh to calculate the potential energy.\n",
      "3. Print out the result.\n",
      "\n",
      "Here is a Python code snippet that accomplishes this:\n",
      "\n",
      "```python\n",
      "# Define given values\n",
      "mass = 10  # in kg\n",
      "height = 5  # in meters\n",
      "gravity = 9.8  # acceleration due to gravity on Earth (m/s^2)\n",
      "\n",
      "# Calculate potential energy using the formula PE = mgh\n",
      "potential_energy = mass * gravity * height\n",
      "\n",
      "print(f\"The potential energy of the object is {potential_energy} Joules.\")\n",
      "```\n",
      "Plan was not approved. Adjusting...\n",
      "[python_worker] Executing: Calculate the potential energy of an object with mass=10kg at a height=5m.\n",
      "Final Code Execution Result: Tool not found: 'python_code'.\n"
     ]
    }
   ],
   "source": [
    "# This file is part of astrochat.\n",
    "#\n",
    "# Developed for the LSST Data Management System.\n",
    "# This product includes software developed by the LSST Project\n",
    "# (https://www.lsst.org).\n",
    "# See the COPYRIGHT file at the top-level directory of this distribution\n",
    "# for details of code ownership.\n",
    "#\n",
    "# This program is free software: you can redistribute it and/or modify\n",
    "# it under the terms of the GNU General Public License as published by\n",
    "# the Free Software Foundation, either version 3 of the License, or\n",
    "# (at your option) any later version.\n",
    "#\n",
    "# This program is distributed in the hope that it will be useful,\n",
    "# but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "# GNU General Public License for more details.\n",
    "#\n",
    "# You should have received a copy of the GNU General Public License\n",
    "# along with this program.  If not, see <https://www.gnu.org/licenses/>.\n",
    "\n",
    "\"\"\"Demo module illustrating an LSST-style Python code layout.\n",
    "\n",
    "This module uses DSPy for LLM-based orchestration, supports memory\n",
    "of user–assistant interactions, implements RAG (Retrieval Augmented\n",
    "Generation) to fetch observational data, and runs Python code via a\n",
    "ProgramOfThought.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "- All docstrings, including these module docstrings, follow the LSST\n",
    "  style constraints for docstrings (Numpydoc).\n",
    "- Comments are kept within 79 characters per line.\n",
    "\"\"\"\n",
    "\n",
    "__all__ = [\n",
    "    \"UserInteraction\",\n",
    "    \"ConversationContext\",\n",
    "    \"ResponseWithContext\",\n",
    "    \"respond_cot\",\n",
    "    \"getObservingData\",\n",
    "    \"retrieve_observing_metadata\",\n",
    "    \"ProgramOfThought\",\n",
    "    \"Tool\",\n",
    "    \"rag_tool\",\n",
    "    \"python_tool\",\n",
    "    \"dummy_tool\",\n",
    "    \"Worker\",\n",
    "    \"rag_worker\",\n",
    "    \"python_worker\",\n",
    "    \"Boss\"\n",
    "]\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "\n",
    "from typing import List, Optional, Callable\n",
    "\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel\n",
    "\n",
    "import dspy\n",
    "from dspy import (\n",
    "    Signature,\n",
    "    ChainOfThought,\n",
    "    Module,\n",
    "    settings\n",
    ")\n",
    "#from dspy.models import LM\n",
    "from dspy.primitives.python_interpreter import CodePrompt, PythonInterpreter\n",
    "\n",
    "LOG = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "#with open('/Users/cmorales/.openaikey.txt', 'r') as file:\n",
    "#    api_key = file.read().strip()\n",
    "\n",
    "lm = dspy.LM(# 'ollama_chat/tulu3:70b', \n",
    "             # 'ollama_chat/olmo2:13b-1124-instruct-fp16',\n",
    "             # 'ollama_chat/olmo2:13b',\n",
    "             # 'ollama_chat/olmo2:7b',\n",
    "             # 'ollama_chat/phi4:14b-fp16',\n",
    "             # 'ollama_chat/dolphin3:latest',\n",
    "             # 'ollama_chat/llama3.3:70b-instruct-q2_K',\n",
    "             # 'ollama_chat/ALIENTELLIGENCE/cybersecuritymonitoring:latest',\n",
    "             # 'ollama_chat/ALIENTELLIGENCE/whiterabbitv2:latest',\n",
    "             # 'ollama_chat/TULU_131k_T08_70b:latest',\n",
    "             # 'ollama_chat/COT_32k_T03_qwq:latest',\n",
    "             # 'ollama_chat/qwq:latest',\n",
    "             # 'ollama_chat/qwq:32b',\n",
    "             # 'ollama_chat/tulu3:8b',\n",
    "             'ollama_chat/tulu3:70b',\n",
    "             # 'ollama_chat/hermes3:3b',\n",
    "             # 'ollama_chat/hermes3:8b',\n",
    "             # 'ollama_chat/hermes3:70b',\n",
    "             # 'ollama_chat/hermes3:405b',\n",
    "             # 'ollama_chat/llama3.3:70b',\n",
    "             # 'ollama_chat/llama3.2-vision:90b',\n",
    "             # 'ollama_chat/llama3.2:3b',\n",
    "             # 'ollama_chat/qwen2.5:0.5b',\n",
    "             # 'ollama_chat/qwen2.5:32b',\n",
    "             # 'ollama_chat/qwen2.5:72b',\n",
    "             # 'ollama_chat/qwen2.5-coder:32b',\n",
    "             # 'ollama_chat/nemotron:70b',\n",
    "             # 'ollama_chat/snowflake-arctic-embed:latest',\n",
    "             # 'ollama_chat/nomic-embed-text:latest',\n",
    "             \n",
    "             api_base='https://5bbc-200-29-147-35.ngrok-free.app', api_key='') #api_key should be empty\n",
    "# lm = dspy.LM('ollama_chat/tulu3:8b', api_base='http://localhost:11434', api_key='') #api_key should be empty\n",
    "# lm = LM(\"openai/gpt-4o\", api_key=api_key)\n",
    "\n",
    "settings.configure(lm=lm)\n",
    "\n",
    "\n",
    "class UserInteraction:\n",
    "    \"\"\"Represents a single user–assistant interaction.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    message : `str`\n",
    "        The user's message.\n",
    "    response : `str`\n",
    "        The assistant's response.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, message: str, response: str):\n",
    "        self.message = message\n",
    "        self.response = response\n",
    "\n",
    "    def serialize_by_role(self) -> List[dict]:\n",
    "        \"\"\"Return the interaction in a format suitable for LLM prompts.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        serialized : `list` of `dict`\n",
    "            List of role/content dicts for user and assistant.\n",
    "        \"\"\"\n",
    "        return [\n",
    "            {\"role\": \"user\", \"content\": self.message},\n",
    "            {\"role\": \"assistant\", \"content\": self.response}\n",
    "        ]\n",
    "\n",
    "\n",
    "class ConversationContext:\n",
    "    \"\"\"Stores and formats recent conversation interactions as context.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    window_size : `int`, optional\n",
    "        Number of recent interactions to keep in memory (default 5).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, window_size: int = 5): \n",
    "        self.window_size = window_size\n",
    "        self.content: List[UserInteraction] = []\n",
    "\n",
    "    def update(self, interaction: UserInteraction) -> None:\n",
    "        \"\"\"Append a new interaction and trim the conversation to `window_size`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        interaction : `UserInteraction`\n",
    "            The new user–assistant interaction to store.\n",
    "        \"\"\"\n",
    "        self.content.append(interaction)\n",
    "        self.content = self.content[-self.window_size:]\n",
    "\n",
    "    def render(self) -> str:\n",
    "        \"\"\"Return a string of the most recent interactions as context.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        context_str : `str`\n",
    "            Conversation text from the most recent interactions.\n",
    "        \"\"\"\n",
    "        out = []\n",
    "        for entry in self.content:\n",
    "            out.append(\n",
    "                f\"User: {entry.message}\\n\\nAssistant: {entry.response}\\n\\n\"\n",
    "            )\n",
    "        return \"\".join(out)\n",
    "\n",
    "\n",
    "class ResponseWithContext(Signature):\n",
    "    \"\"\"Signature requiring context, user message, and RAG context.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    context : `str`\n",
    "        Conversation context.\n",
    "    message : `str`\n",
    "        The user's message.\n",
    "    rag_context : `str`\n",
    "        Information from retrieval-augmented generation (RAG).\n",
    "    response : `str`\n",
    "        The contextualized response (output).\n",
    "    \"\"\"\n",
    "    context = dspy.InputField(desc=\"Conversation context\")\n",
    "    message = dspy.InputField(desc=\"User message\")\n",
    "    rag_context = dspy.InputField(\n",
    "        desc=\"Relevant information from external or observational data\"\n",
    "    )\n",
    "    response = dspy.OutputField(desc=\"Contextualized response\")\n",
    "\n",
    "\n",
    "respond_cot = ChainOfThought(ResponseWithContext)\n",
    "\n",
    "\n",
    "def getObservingData(dayObs: Optional[int] = None) -> pd.DataFrame:\n",
    "    \"\"\"Obtain a DataFrame of observational data (dummy example).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dayObs : `int` or `None`, optional\n",
    "        A day-identifier for observational data. If None, default to\n",
    "        a placeholder value.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : `pandas.DataFrame`\n",
    "        A DataFrame sorted by its index with columns from the JSON file.\n",
    "        May be empty if file is not found or an error occurs.\n",
    "    \"\"\"\n",
    "    localDayObs = 20230913  # dummy placeholder\n",
    "    if dayObs is None:\n",
    "        dayObs = localDayObs\n",
    "\n",
    "    filename = f\"/home/c/carlosm/lsst/summit_extras/python/lsst/summit/extras/dayObs_{dayObs}.json\"\n",
    "    if not os.path.exists(filename):\n",
    "        LOG.warning(\"File not found: %s\", filename)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    try:\n",
    "        df = pd.read_json(filename).T\n",
    "        # Drop columns starting with underscore\n",
    "        df = df.drop(\n",
    "            [col for col in df.columns if col.startswith(\"_\")],\n",
    "            axis=1,\n",
    "            errors=\"ignore\"\n",
    "        )\n",
    "        return df.sort_index()\n",
    "    except Exception as exc:\n",
    "        LOG.error(\"Error reading %s: %s\", filename, exc)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def retrieve_observing_metadata(query: str, dayObs: Optional[int] = None) -> str:\n",
    "    \"\"\"Filter a DataFrame by a query and return matching rows.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    query : `str`\n",
    "        Substring to search within the DataFrame's string representation.\n",
    "    dayObs : `int` or `None`, optional\n",
    "        Observation day identifier. Defaults to None for placeholder data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    result_str : `str`\n",
    "        String representation of matching rows, or a message if none found.\n",
    "    \"\"\"\n",
    "    df = getObservingData(dayObs)\n",
    "    if df.empty:\n",
    "        return \"No observing data available.\"\n",
    "\n",
    "    # Filter rows containing `query`\n",
    "    result = df[df.apply(\n",
    "        lambda row: query.lower() in row.to_string().lower(), axis=1\n",
    "    )]\n",
    "    if result.empty:\n",
    "        return \"No relevant data found.\"\n",
    "    return result.to_string(index=False)\n",
    "\n",
    "\n",
    "class ProgramOfThought(Module):\n",
    "    \"\"\"Generates and executes Python code iteratively up to `max_iters`.\n",
    "\n",
    "    This uses a `PythonInterpreter` with an import whitelist\n",
    "    to control allowed packages.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    signature : `dspy.Signature`\n",
    "        DSpy signature specifying input/output fields.\n",
    "    max_iters : `int`, optional\n",
    "        Maximum number of code generation attempts (default 3).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, signature: Signature, max_iters: int = 3):\n",
    "        super().__init__()\n",
    "        self.signature = signature\n",
    "        self.max_iters = max_iters\n",
    "        self.interpreter = PythonInterpreter(\n",
    "            action_space={\"print\": print},\n",
    "            import_white_list=[\"numpy\", \"astropy\", \"sympy\", \"matplotlib\"]\n",
    "        )\n",
    "\n",
    "    def forward(self, question: str, context_str: str = \"\",\n",
    "                rag_context: str = \"\", variables: dict = None) -> dict:\n",
    "        \"\"\"Generate and execute Python code for `question`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        question : `str`\n",
    "            The user question or request to be solved via Python code.\n",
    "        context_str : `str`, optional\n",
    "            Conversation context to include in code generation.\n",
    "        rag_context : `str`, optional\n",
    "            Data from RAG to include in code generation.\n",
    "        variables : `dict`, optional\n",
    "            Additional variables or context.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        result_dict : `dict`\n",
    "            Dictionary with key \"answer\" containing the code execution\n",
    "            result or None if unsuccessful.\n",
    "        \"\"\"\n",
    "        if variables is None:\n",
    "            variables = {}\n",
    "\n",
    "        for iteration in range(self.max_iters):\n",
    "            generated_code = self._generate_code(question, context_str,\n",
    "                                                 rag_context)\n",
    "            print(f\"\\nIteration {iteration + 1}: Generated Code:\\n\"\n",
    "                  f\"{generated_code}\")\n",
    "\n",
    "            try:\n",
    "                result, _ = CodePrompt(generated_code).execute(\n",
    "                    self.interpreter, user_variable=variables\n",
    "                )\n",
    "                print(\"Execution Result:\", result)\n",
    "                return {\"answer\": result}\n",
    "            except Exception as exc:\n",
    "                LOG.error(\"Execution Error: %s\", exc)\n",
    "\n",
    "        LOG.error(\"Failed to generate valid code after multiple attempts.\")\n",
    "        return {\"answer\": None}\n",
    "\n",
    "    def _generate_code(self, question: str, context_str: str,\n",
    "                       rag_context: str) -> str:\n",
    "        \"\"\"Helper method to produce Python code via LLM.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        question : `str`\n",
    "            The user question or request.\n",
    "        context_str : `str`\n",
    "            Relevant conversation context.\n",
    "        rag_context : `str`\n",
    "            Retrieved data (RAG).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        code : `str`\n",
    "            Cleaned and ready-to-run Python code.\n",
    "        \"\"\"\n",
    "        code_signature = dspy.Signature(\"question -> generated_code\")\n",
    "        predict = dspy.Predict(code_signature)\n",
    "\n",
    "        prompt = (\n",
    "            \"You are an AI that writes Python code to solve problems.\\n\"\n",
    "            \"Below is some conversation context and retrieved metadata:\\n\\n\"\n",
    "            f\"Context:\\n{context_str}\\n\\n\"\n",
    "            f\"Observing Metadata:\\n{rag_context}\\n\\n\"\n",
    "            \"Now, generate Python code to solve the following problem.\\n\"\n",
    "            \"Do not include any explanations or markdown.\\n\"\n",
    "            \"Only output executable Python code.\\n\\n\"\n",
    "            f\"Question: {question}\\n\\nCode:\\n\"\n",
    "        )\n",
    "        completion = predict(question=prompt)\n",
    "        code_block = re.sub(r\"```[^\\n]*\\n\", \"\", completion.generated_code)\n",
    "        code_block = re.sub(r\"```\", \"\", code_block)\n",
    "        return code_block\n",
    "\n",
    "\n",
    "class Tool(BaseModel):\n",
    "    \"\"\"Base template for Tools used by Worker modules.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    name : `str`\n",
    "        Name of the tool.\n",
    "    description : `str`\n",
    "        Short description of the tool's function.\n",
    "    requires : `str`\n",
    "        Description of the required input parameter(s).\n",
    "    func : `Callable`\n",
    "        The callable used to perform the tool's operation.\n",
    "    \"\"\"\n",
    "\n",
    "    name: str\n",
    "    description: str\n",
    "    requires: str\n",
    "    func: Callable\n",
    "\n",
    "\n",
    "rag_tool = Tool(\n",
    "    name=\"rag retrieval\",\n",
    "    description=\"Search observation data for a query and return results.\",\n",
    "    requires=\"query\",\n",
    "    func=lambda q: retrieve_observing_metadata(q)\n",
    ")\n",
    "\n",
    "code_tool_signature = dspy.Signature(\"question -> answer\")\n",
    "code_program = ProgramOfThought(signature=code_tool_signature, max_iters=3)\n",
    "\n",
    "python_tool = Tool(\n",
    "    name=\"python executor\",\n",
    "    description=(\"Generate and execute Python code with context, \"\n",
    "                 \"using ProgramOfThought.\"),\n",
    "    requires=\"question\",\n",
    "    func=lambda question_dict: code_program(\n",
    "        question=question_dict.get(\"question\", \"\"),\n",
    "        context_str=question_dict.get(\"context\", \"\"),\n",
    "        rag_context=question_dict.get(\"rag_context\", \"\"),\n",
    "        variables=question_dict.get(\"variables\", {})\n",
    "    )\n",
    ")\n",
    "\n",
    "dummy_tool = Tool(\n",
    "    name=\"dummy\",\n",
    "    description=\"Example tool for future extension.\",\n",
    "    requires=\"some_argument\",\n",
    "    func=lambda arg: f\"Dummy tool called with argument: {arg}\"\n",
    ")\n",
    "\n",
    "\n",
    "class Worker(Module):\n",
    "    \"\"\"Generic worker that can plan tasks, decide on tools, and execute them.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    role : `str`\n",
    "        Role or name of the worker (e.g., \"rag_worker\").\n",
    "    tools : `list` of `Tool`\n",
    "        List of available Tool instances.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, role: str, tools: List[Tool]):\n",
    "        self.role = role\n",
    "        self.tools = {t.name: t for t in tools}\n",
    "        self._plan = ChainOfThought(\"task, context -> proposed_plan\")\n",
    "        self._use_tool = ChainOfThought(\"task, context -> tool_name, tool_argument\")\n",
    "\n",
    "    def plan(self, task: str, feedback: str = \"\") -> str:\n",
    "        \"\"\"Generate a textual plan for the given `task`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        task : `str`\n",
    "            Task description or goal.\n",
    "        feedback : `str`, optional\n",
    "            Feedback from a supervisor or orchestration component.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        plan_text : `str`\n",
    "            Proposed plan as text.\n",
    "        \"\"\"\n",
    "        context = (\n",
    "            f\"Worker role: {self.role}\\n\"\n",
    "            f\"Tools: {list(self.tools.keys())}\\n\"\n",
    "            f\"Feedback:\\n{feedback}\"\n",
    "        )\n",
    "        result = self._plan(task=task, context=context)\n",
    "        return result.proposed_plan\n",
    "\n",
    "    def execute(self, task: str, use_tool: bool, context: str = \"\") -> str:\n",
    "        \"\"\"Execute the `task`, optionally using a tool.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        task : `str`\n",
    "            The task or instruction to execute.\n",
    "        use_tool : `bool`\n",
    "            Whether to use a tool or not.\n",
    "        context : `str`, optional\n",
    "            Plan or context to pass to the tool decision logic.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        outcome : `str`\n",
    "            Result of the execution or an error message.\n",
    "        \"\"\"\n",
    "        print(f\"[{self.role}] Executing: {task}\")\n",
    "        if not use_tool:\n",
    "            return f\"Task '{task}' completed without tools.\"\n",
    "\n",
    "        tool_decision = self._use_tool(task=task, context=context)\n",
    "        tool_name = tool_decision.tool_name\n",
    "        arg = tool_decision.tool_argument\n",
    "\n",
    "        if tool_name in self.tools:\n",
    "            return self.tools[tool_name].func(arg)\n",
    "\n",
    "        return f\"Tool not found: '{tool_name}'.\"\n",
    "\n",
    "\n",
    "rag_worker = Worker(\"rag_worker\", tools=[rag_tool, dummy_tool])\n",
    "python_worker = Worker(\"python_worker\", tools=[python_tool, dummy_tool])\n",
    "\n",
    "\n",
    "class Boss(Module):\n",
    "    \"\"\"Orchestrates tasks by assigning them to Workers and managing their plans.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    workers : `list` of `Worker`\n",
    "        List of workers available to the boss.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, workers: List[Worker]):\n",
    "        self.workers = {w.role: w for w in workers}\n",
    "        self._assign = ChainOfThought(\"task -> who\")\n",
    "        self._approve = ChainOfThought(\"task, plan -> approval\")\n",
    "\n",
    "    def plan_and_execute(self, task: str, worker_hint: str = \"\",\n",
    "                         use_tool: bool = True) -> str:\n",
    "        \"\"\"Decide which worker to use, generate and approve a plan, then execute.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        task : `str`\n",
    "            The goal or instruction to achieve.\n",
    "        worker_hint : `str`, optional\n",
    "            Suggestion for which worker to assign (bypasses assignment logic).\n",
    "        use_tool : `bool`, optional\n",
    "            Whether the chosen worker should use a tool (default True).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        exec_result : `str`\n",
    "            The result of the worker's execution.\n",
    "        \"\"\"\n",
    "        if worker_hint and worker_hint in self.workers:\n",
    "            assignee = worker_hint\n",
    "        else:\n",
    "            assign_result = self._assign(task=task)\n",
    "            assignee = assign_result.who\n",
    "\n",
    "        if assignee not in self.workers:\n",
    "            return f\"No worker found with role '{assignee}'.\"\n",
    "\n",
    "        worker = self.workers[assignee]\n",
    "        plan_text = worker.plan(task, feedback=\"N/A\")\n",
    "        print(f\"Proposed Plan: {plan_text}\")\n",
    "\n",
    "        approval_res = self._approve(task=task, plan=plan_text)\n",
    "        is_approved = \"yes\" in approval_res.approval.lower()\n",
    "        if not is_approved:\n",
    "            # TODO: DM-12345 Possibly refine or iterate plan if disapproved\n",
    "            print(\"Plan was not approved. Adjusting...\")\n",
    "\n",
    "        context = f\"Plan: {plan_text}\\nBoss: Approved\"\n",
    "        exec_result = worker.execute(task, use_tool=use_tool, context=context)\n",
    "        return exec_result\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Demo main execution function for illustration.\"\"\"\n",
    "    boss = Boss(workers=[rag_worker, python_worker])\n",
    "\n",
    "    # Example 1: Retrieve data about 'spectra' using RAG\n",
    "    print(\"\\n=== EXAMPLE 1: RAG ===\")\n",
    "    result_1 = boss.plan_and_execute(\n",
    "        task=\"Find observation data about spectra.\",\n",
    "        worker_hint=\"rag_worker\",\n",
    "        use_tool=True\n",
    "    )\n",
    "    print(\"Final RAG Result:\", result_1)\n",
    "\n",
    "    # Example 2: Run Python code to compute potential energy\n",
    "    print(\"\\n=== EXAMPLE 2: Python Code Execution ===\")\n",
    "    result_2 = boss.plan_and_execute(\n",
    "        task=\"Calculate the potential energy of an object with mass=10kg \"\n",
    "             \"at a height=5m.\",\n",
    "        worker_hint=\"python_worker\",\n",
    "        use_tool=True\n",
    "    )\n",
    "    print(\"Final Code Execution Result:\", result_2)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSPy-dantic-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
