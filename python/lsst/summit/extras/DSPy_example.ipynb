{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949731ed-3932-483e-9325-d2058a1405cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "import re\n",
    "from dspy.primitives.python_interpreter import PythonInterpreter, CodePrompt\n",
    "\n",
    "# Configure a local model with Ollama\n",
    "# lm = dspy.LM('ollama_chat/tulu3', api_base='https://xxxx-xxx-xx-xxx-xx.ngrok-free.app', api_key='') #api_key should be empty\n",
    "# lm = dspy.LM('ollama_chat/tulu3', api_base='http://localhost:11434', api_key='') #api_key should be empty\n",
    "\n",
    "# Read the API key from a file\n",
    "with open('/home/c/carlosm/.openaikey.txt', 'r') as file:\n",
    "    api_key = file.read().strip()\n",
    "\n",
    "# Configure the language model with OpenAI GPT-4o\n",
    "lm = dspy.LM('openai/gpt-4o', api_key=api_key)\n",
    "dspy.settings.configure(lm=lm)\n",
    "\n",
    "\n",
    "class ProgramOfThought(dspy.Module):\n",
    "    \"\"\"Class to generate and execute Python code iteratively to solve questions.\"\"\"\n",
    "\n",
    "    def __init__(self, signature, max_iters=3):\n",
    "        super().__init__()\n",
    "        self.signature = signature\n",
    "        self.max_iters = max_iters\n",
    "        # Allow importing only numpy, astropy, and sympy\n",
    "        self.interpreter = PythonInterpreter(\n",
    "            action_space={\"print\": print},\n",
    "            import_white_list=[\"numpy\", \"astropy\", \"sympy\"]\n",
    "        )\n",
    "\n",
    "    def forward(self, question):\n",
    "        \"\"\"Generates and executes Python code for a given question up to a maximum number of iterations.\"\"\"\n",
    "\n",
    "        for iteration in range(self.max_iters):\n",
    "            # Generate Python code for the question\n",
    "            generated_code = self._generate_code(question)\n",
    "            print(f\"\\nIteration {iteration + 1}:\")\n",
    "            print(\"Generated Code:\\n\", generated_code)\n",
    "\n",
    "            # Attempt to execute the code\n",
    "            try:\n",
    "                result, _ = CodePrompt(generated_code).execute(self.interpreter)\n",
    "                print(\"Execution Result:\\n\", result)\n",
    "                return {\"answer\": result}\n",
    "            except Exception as e:\n",
    "                print(\"Execution Error:\\n\", e)\n",
    "\n",
    "        # Return None if all iterations fail\n",
    "        print(\"\\nFailed to generate valid code after several iterations.\")\n",
    "        return {\"answer\": None}\n",
    "\n",
    "    def _generate_code(self, question):\n",
    "        \"\"\"Generates Python code using the model for the given question.\"\"\"\n",
    "\n",
    "        generate_code_signature = dspy.Signature(\"question -> generated_code\")\n",
    "        predict = dspy.Predict(generate_code_signature)\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "You are an AI that writes Python code to solve problems. \n",
    "Generate Python code to solve the following problem. Do not include any explanations, Markdown formatting, or comments. \n",
    "Only output the executable Python code.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Code:\n",
    "\"\"\"\n",
    "        # Use the model to generate the code\n",
    "        completion = predict(question=prompt)\n",
    "        generated_code = completion.generated_code\n",
    "\n",
    "        # Clean code block tags like ```python and ```\n",
    "        cleaned_code = re.sub(r\"```[^\\n]*\\n\", \"\", generated_code)\n",
    "        cleaned_code = re.sub(r\"```\", \"\", cleaned_code)\n",
    "\n",
    "        return cleaned_code\n",
    "\n",
    "\n",
    "# Create an instance of ProgramOfThought with a simple signature\n",
    "generate_answer_signature = dspy.Signature(\"question -> answer\")\n",
    "pot = ProgramOfThought(signature=generate_answer_signature)\n",
    "\n",
    "# Example usage\n",
    "question = \"Chris saw 2 viscachas at lunch time, then 3 at dinner. How many viscachas did Chris see in the day?\"\n",
    "result = pot(question=question)\n",
    "\n",
    "# Print the final answer\n",
    "print(\"\\nQuestion:\", question)\n",
    "print(\"Final Answer:\", result['answer'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c67b95-b2ba-4168-8317-880b62e8fd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another example\n",
    "question = (\n",
    "    \"An object in free fall travels a distance y(t) = 5t^2 meters as a function of time t in seconds. \"\n",
    "    \"What is the object's acceleration and velocity at any instant t= 2.5?\"\n",
    ")\n",
    "result = pot(question=question)\n",
    "\n",
    "# Print the final answer\n",
    "print(\"\\nQuestion:\", question)\n",
    "print(\"Final Answer:\", result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7c8ca8-99f8-4a1b-9fd3-6733fb7105b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d637c9d2-743e-484d-af5e-f819beb44688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another example\n",
    "question = \"Using the astropy library, convert 1.5 parsecs to light-years.\"\n",
    "result = pot(question=question)\n",
    "\n",
    "# Print the final answer\n",
    "print(\"\\nQuestion:\", question)\n",
    "print(\"Final Answer:\", result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3d1ed1-c639-4ca1-8c13-9c3337e143d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another example\n",
    "question = \"A celestial object has equatorial coordinates of right ascension 10 hours and declination +20 degrees. Convert these coordinates to galactic coordinates using astropy.\"\n",
    "result = pot(question=question)\n",
    "\n",
    "# Print the final answer\n",
    "print(\"\\nQuestion:\", question)\n",
    "print(\"Final Answer:\", result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af75917f-ea1b-4aba-813b-a6e1d7c9931e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another example\n",
    "question = \"A celestial object moves at a speed of 30,000 km/s. Calculate the relativistic redshift of this object using astropy.\"\n",
    "result = pot(question=question)\n",
    "\n",
    "# Print the final answer\n",
    "print(\"\\nQuestion:\", question)\n",
    "print(\"Final Answer:\", result['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f3de18-2db5-4947-a7d9-644cdd717693",
   "metadata": {},
   "source": [
    "## Memory Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891aecda-6856-4a4e-ad69-f8303eae44bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "import re\n",
    "from dspy.primitives.python_interpreter import PythonInterpreter, CodePrompt\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Configure a local model with Ollama\n",
    "# lm = dspy.LM('ollama_chat/tulu3', api_base='https://xxxx-xxx-xx-xxx-xx.ngrok-free.app', api_key='') #api_key should be empty\n",
    "# lm = dspy.LM('ollama_chat/tulu3', api_base='http://localhost:11434', api_key='') #api_key should be empty\n",
    "\n",
    "# Read the API key from a file\n",
    "with open('/home/c/carlosm/.openaikey.txt', 'r') as file:\n",
    "    api_key = file.read().strip()\n",
    "\n",
    "# Configure the language model with OpenAI GPT-4o\n",
    "lm = dspy.LM('openai/gpt-4o', api_key=api_key)\n",
    "dspy.settings.configure(lm=lm)\n",
    "\n",
    "# =====================\n",
    "# Conversation Memory Classes\n",
    "# =====================\n",
    "\n",
    "class UserInteraction(BaseModel):\n",
    "    message: str = None\n",
    "    response: str = None\n",
    "\n",
    "    def serialize_by_role(self):\n",
    "        return [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"{self.message}\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": f\"{self.response}\"\n",
    "            }\n",
    "\n",
    "        ]\n",
    "\n",
    "class ConversationContext(BaseModel):\n",
    "    window_size: int = 3\n",
    "    content: list[UserInteraction] = []\n",
    "\n",
    "    @staticmethod\n",
    "    def _format_interaction(interaction):\n",
    "        return f\"User: {interaction.message}\\n\\nAssistant: {interaction.response}\\n\\n\"\n",
    "\n",
    "    def render(self):\n",
    "        formatted = [\n",
    "            self._format_interaction(interaction)\n",
    "            for interaction in self.content\n",
    "        ]\n",
    "        return \"\".join(formatted)\n",
    "\n",
    "    def update(self, interaction: UserInteraction):\n",
    "        # Keep only the last `window_size` interactions\n",
    "        self.content = self.content[-self.window_size:] + [interaction]\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.render()\n",
    "\n",
    "\n",
    "# =====================\n",
    "# Signature and ChainOfThought for Responding with Context\n",
    "# =====================\n",
    "\n",
    "class ResponseWithContext(dspy.Signature):\n",
    "    \"\"\"Respond to the user's message given some conversation context\"\"\"\n",
    "    context = dspy.InputField(desc=\"The context of the conversation\")\n",
    "    message = dspy.InputField(desc=\"The user's message\")\n",
    "    response = dspy.OutputField(desc=\"A useful response to the user's message within the context of the conversation\")\n",
    "\n",
    "respond_cot = dspy.ChainOfThought(ResponseWithContext)\n",
    "\n",
    "\n",
    "# =====================\n",
    "# ProgramOfThought Class (as before, but now including context)\n",
    "# =====================\n",
    "\n",
    "class ProgramOfThought(dspy.Module):\n",
    "    \"\"\"Class to generate and execute Python code iteratively to solve questions, now with context consideration.\"\"\"\n",
    "\n",
    "    def __init__(self, signature, max_iters=3):\n",
    "        super().__init__()\n",
    "        self.signature = signature\n",
    "        self.max_iters = max_iters\n",
    "        # Allow importing only numpy, astropy, and sympy\n",
    "        self.interpreter = PythonInterpreter(\n",
    "            action_space={\"print\": print},\n",
    "            import_white_list=[\"numpy\", \"astropy\", \"sympy\", \"matplotlib\", \"mpmath\"],\n",
    "        )\n",
    "\n",
    "    def forward(self, question, variables=None, context_str=\"\"):\n",
    "        \"\"\"Generates and executes Python code for a given question up to a maximum number of iterations, using provided context for better answers.\"\"\"\n",
    "        for iteration in range(self.max_iters):\n",
    "            # Generate Python code for the question, including context\n",
    "            generated_code = self._generate_code(question, context_str)\n",
    "            print(f\"\\nIteration {iteration + 1}:\")\n",
    "            print(\"Generated Code:\\n\", generated_code)\n",
    "\n",
    "            # Attempt to execute the code with the provided variables\n",
    "            try:\n",
    "                result, _ = CodePrompt(generated_code).execute(self.interpreter, user_variable=variables)\n",
    "                print(\"Execution Result:\\n\", result)\n",
    "                return {\"answer\": result}\n",
    "            except Exception as e:\n",
    "                print(\"Execution Error:\\n\", e)\n",
    "\n",
    "        # Return None if all iterations fail\n",
    "        print(\"\\nFailed to generate valid code after several iterations.\")\n",
    "        return {\"answer\": None}\n",
    "\n",
    "    def _generate_code(self, question, context_str=\"\"):\n",
    "        \"\"\"Generates Python code using the model for the given question, with additional conversation context.\"\"\"\n",
    "        generate_code_signature = dspy.Signature(\"question -> generated_code\")\n",
    "        predict = dspy.Predict(generate_code_signature)\n",
    "\n",
    "        # We incorporate the conversation context into the prompt:\n",
    "        # The model can use the previous Q&A to provide a better solution.\n",
    "        prompt = f\"\"\"\n",
    "You are an AI that writes Python code to solve problems. \n",
    "Below is the conversation context so far:\n",
    "\n",
    "{context_str}\n",
    "\n",
    "Now, generate Python code to solve the following problem. Do not include any explanations, Markdown formatting, or comments. \n",
    "Only output the executable Python code.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Code:\n",
    "\"\"\"\n",
    "        # Use the model to generate the code\n",
    "        completion = predict(question=prompt)\n",
    "        generated_code = completion.generated_code\n",
    "\n",
    "        # Clean code block tags like ```python and ```\n",
    "        cleaned_code = re.sub(r\"```[^\\n]*\\n\", \"\", generated_code)\n",
    "        cleaned_code = re.sub(r\"```\", \"\", cleaned_code)\n",
    "\n",
    "        return cleaned_code\n",
    "\n",
    "\n",
    "# =====================\n",
    "# Example main loop integrating memory\n",
    "# =====================\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Create conversation context with a window size\n",
    "    context = ConversationContext(window_size=5)\n",
    "\n",
    "    # Create an instance of ProgramOfThought with a simple signature\n",
    "    generate_answer_signature = dspy.Signature(\"question -> answer\")\n",
    "    pot = ProgramOfThought(signature=generate_answer_signature)\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            user_message = input(\">>> \")\n",
    "\n",
    "            if user_message == ':exit':\n",
    "                break\n",
    "            elif user_message == ':history':\n",
    "                print(context.render())\n",
    "                continue\n",
    "\n",
    "            # Use respond_cot to get an assistant response based on the conversation history\n",
    "            # This response simulates a \"chain of thought\" for a normal conversational turn\n",
    "            assistant_response = respond_cot(\n",
    "                context=context.render(),\n",
    "                message=user_message\n",
    "            ).response\n",
    "\n",
    "            # Print the assistant response\n",
    "            print(f'\\n<<< {assistant_response}\\n')\n",
    "\n",
    "            # Update the conversation context\n",
    "            interaction = UserInteraction(message=user_message, response=assistant_response)\n",
    "            context.update(interaction)\n",
    "\n",
    "            # Now, if the user message is a problem that requires code generation,\n",
    "            # we can call pot using the current conversation context.\n",
    "            # For example, if the user asks a physics question:\n",
    "            if \"calculate\" in user_message.lower() or \"solve\" in user_message.lower():\n",
    "                variables = {\"gravity\": 9.81}  # Example variable\n",
    "                result = pot(question=user_message, variables=variables, context_str=context.render())\n",
    "                print(\"ProgramOfThought Answer:\", result['answer'])\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            break\n",
    "\n",
    "    print('\\nBye!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1d74f9-9ebb-45f6-9178-616e9e48e480",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T15:33:03.287663Z",
     "iopub.status.busy": "2025-01-08T15:33:03.287538Z",
     "iopub.status.idle": "2025-01-08T15:33:03.289709Z",
     "shell.execute_reply": "2025-01-08T15:33:03.289370Z",
     "shell.execute_reply.started": "2025-01-08T15:33:03.287651Z"
    }
   },
   "source": [
    "## RAG Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a858a469",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T15:33:22.823792Z",
     "iopub.status.busy": "2025-01-08T15:33:22.823492Z",
     "iopub.status.idle": "2025-01-08T15:34:09.443592Z",
     "shell.execute_reply": "2025-01-08T15:34:09.443158Z",
     "shell.execute_reply.started": "2025-01-08T15:33:22.823775Z"
    }
   },
   "outputs": [],
   "source": [
    "import dspy\n",
    "import re\n",
    "import pandas as pd\n",
    "from dspy.primitives.python_interpreter import PythonInterpreter, CodePrompt\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Configure a local model with Ollama\n",
    "# lm = dspy.LM('ollama_chat/tulu3', api_base='https://xxxx-xxx-xx-xxx-xx.ngrok-free.app', api_key='') #api_key should be empty\n",
    "# lm = dspy.LM('ollama_chat/tulu3', api_base='http://localhost:11434', api_key='') #api_key should be empty\n",
    "\n",
    "# Read the API key from a file\n",
    "with open('/home/c/carlosm/.openaikey.txt', 'r') as file:\n",
    "    api_key = file.read().strip()\n",
    "\n",
    "# Configure the language model with OpenAI GPT-4o\n",
    "lm = dspy.LM('openai/gpt-4o', api_key=api_key)\n",
    "dspy.settings.configure(lm=lm)\n",
    "\n",
    "LOG = logging.getLogger(__name__)\n",
    "# =====================\n",
    "# Conversation Memory Classes\n",
    "# =====================\n",
    "\n",
    "class UserInteraction(BaseModel):\n",
    "    message: str = None\n",
    "    response: str = None\n",
    "\n",
    "    def serialize_by_role(self):\n",
    "        return [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"{self.message}\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": f\"{self.response}\"\n",
    "            }\n",
    "        ]\n",
    "\n",
    "\n",
    "class ConversationContext(BaseModel):\n",
    "    window_size: int = 3\n",
    "    content: list[UserInteraction] = []\n",
    "\n",
    "    @staticmethod\n",
    "    def _format_interaction(interaction):\n",
    "        return f\"User: {interaction.message}\\n\\nAssistant: {interaction.response}\\n\\n\"\n",
    "\n",
    "    def render(self):\n",
    "        formatted = [\n",
    "            self._format_interaction(interaction)\n",
    "            for interaction in self.content\n",
    "        ]\n",
    "        return \"\".join(formatted)\n",
    "\n",
    "    def update(self, interaction: UserInteraction):\n",
    "        self.content = self.content[-self.window_size:] + [interaction]\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.render()\n",
    "\n",
    "\n",
    "# =====================\n",
    "# Signature and ChainOfThought for Responding with Context\n",
    "# =====================\n",
    "\n",
    "class ResponseWithContext(dspy.Signature):\n",
    "    \"\"\"Respond to the user's message given some conversation context.\"\"\"\n",
    "    context = dspy.InputField(desc=\"The context of the conversation\")\n",
    "    message = dspy.InputField(desc=\"The user's message\")\n",
    "    rag_context = dspy.InputField(desc=\"Relevant data from observing metadata\")\n",
    "    response = dspy.OutputField(desc=\"A useful response to the user's message within the context of the conversation\")\n",
    "\n",
    "respond_cot = dspy.ChainOfThought(ResponseWithContext)\n",
    "\n",
    "\n",
    "# =====================\n",
    "# Retrieval Function (RAG)\n",
    "# =====================\n",
    "def getObservingData(dayObs=None):\n",
    "    \"\"\"Get the observing metadata for the current or a past day.\n",
    "\n",
    "    Get the observing metadata for the current or a past day. The metadata\n",
    "    is the contents of the table on RubinTV. If a day is not specified, the\n",
    "    current day is used. The metadata is returned as a pandas dataframe.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dayObs : `int`, optional\n",
    "        The day for which to get the observing metadata. If not specified,\n",
    "        the current day is used.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    observingData: `pandas.DataFrame`\n",
    "        The observing metadata for the specified day.\n",
    "    \"\"\"\n",
    "    currentDayObs = 20240904\n",
    "    if dayObs is None:\n",
    "        dayObs = currentDayObs\n",
    "    isCurrent = dayObs == currentDayObs\n",
    "\n",
    "    site =  \"local\" #getSite()\n",
    "\n",
    "    filename = None\n",
    "    if site == \"local\":\n",
    "        filename = f\"/home/c/carlosm/lsst/summit_extras/python/lsst/summit/extras/dayObs_20240904.json\"\n",
    "    elif site in [\"rubin-devl\"]:\n",
    "        LOG.warning(\n",
    "            f\"Observing metadata at {site} is currently copied by hand by Merlin and will\"\n",
    "            \" not be updated in realtime\"\n",
    "        )\n",
    "        filename = f\"/sdf/home/m/mfl/u/rubinTvDataProducts/sidecar_metadata/dayObs_{dayObs}.json\"\n",
    "    elif site in [\"staff-rsp\"]:\n",
    "        LOG.warning(\n",
    "            f\"Observing metadata at {site} is currently copied by hand by Merlin and will\"\n",
    "            \" not be updated in realtime\"\n",
    "        )\n",
    "        filename = f\"/home/m/mfl/u/rubinTvDataProducts/sidecar_metadata/dayObs_{dayObs}.json\"\n",
    "    else:\n",
    "        raise RuntimeError(f\"Observing metadata not available for site {site}\")\n",
    "\n",
    "    # check the file exists, and raise if not\n",
    "    if not os.path.exists(filename):\n",
    "        LOG.warning(\n",
    "            f\"Observing metadata file for {'current' if isCurrent else ''} dayObs \"\n",
    "            f\"{dayObs} does not exist at {filename}.\"\n",
    "        )\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    table = pd.read_json(filename).T\n",
    "    table = table.sort_index()\n",
    "\n",
    "    # remove all the columns which start with a leading underscore, as these\n",
    "    # are used by the backend to signal how specific cells should be colored\n",
    "    # on RubinTV, and for nothing else.\n",
    "    table = table.drop([col for col in table.columns if col.startswith(\"_\")], axis=1)\n",
    "\n",
    "    return table\n",
    "\n",
    "def retrieve_observing_metadata(query, dayObs=None):\n",
    "    \"\"\"Retrieve relevant observing data based on the query.\"\"\"\n",
    "    df = getObservingData(dayObs=dayObs)\n",
    "    if df.empty:\n",
    "        return \"No observing data available.\"\n",
    "    \n",
    "    # Filter or search the dataframe based on the query\n",
    "    result = df[df.apply(lambda row: query.lower() in row.to_string().lower(), axis=1)]\n",
    "    return result.to_string(index=False) if not result.empty else \"No relevant data found.\"\n",
    "\n",
    "\n",
    "# =====================\n",
    "# ProgramOfThought Class (Updated for RAG)\n",
    "# =====================\n",
    "\n",
    "class ProgramOfThought(dspy.Module):\n",
    "    \"\"\"Class to generate and execute Python code iteratively to solve questions, now with context consideration.\"\"\"\n",
    "\n",
    "    def __init__(self, signature, max_iters=3):\n",
    "        super().__init__()\n",
    "        self.signature = signature\n",
    "        self.max_iters = max_iters\n",
    "        self.interpreter = PythonInterpreter(\n",
    "            action_space={\"print\": print},\n",
    "            import_white_list=[\"numpy\", \"astropy\", \"sympy\", \"matplotlib\"],\n",
    "        )\n",
    "\n",
    "    def forward(self, question, variables=None, context_str=\"\", rag_context=\"\"):\n",
    "        \"\"\"Generates and executes Python code for a given question.\"\"\"\n",
    "        for iteration in range(self.max_iters):\n",
    "            generated_code = self._generate_code(question, context_str, rag_context)\n",
    "            print(f\"\\nIteration {iteration + 1}:\")\n",
    "            print(\"Generated Code:\\n\", generated_code)\n",
    "\n",
    "            try:\n",
    "                result, _ = CodePrompt(generated_code).execute(self.interpreter, user_variable=variables)\n",
    "                print(\"Execution Result:\\n\", result)\n",
    "                return {\"answer\": result}\n",
    "            except Exception as e:\n",
    "                print(\"Execution Error:\\n\", e)\n",
    "\n",
    "        print(\"\\nFailed to generate valid code after several iterations.\")\n",
    "        return {\"answer\": None}\n",
    "\n",
    "    def _generate_code(self, question, context_str=\"\", rag_context=\"\"):\n",
    "        \"\"\"Generates Python code using the model with RAG data.\"\"\"\n",
    "        generate_code_signature = dspy.Signature(\"question -> generated_code\")\n",
    "        predict = dspy.Predict(generate_code_signature)\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "You are an AI that writes Python code to solve problems. \n",
    "Below is the conversation context and relevant observing metadata:\n",
    "\n",
    "Context:\n",
    "{context_str}\n",
    "\n",
    "Observing Metadata:\n",
    "{rag_context}\n",
    "\n",
    "Now, generate Python code to solve the following problem. Do not include any explanations, Markdown formatting, or comments. \n",
    "Only output the executable Python code.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Code:\n",
    "\"\"\"\n",
    "        completion = predict(question=prompt)\n",
    "        generated_code = completion.generated_code\n",
    "\n",
    "        cleaned_code = re.sub(r\"```[^\\n]*\\n\", \"\", generated_code)\n",
    "        cleaned_code = re.sub(r\"```\", \"\", cleaned_code)\n",
    "\n",
    "        return cleaned_code\n",
    "\n",
    "\n",
    "# =====================\n",
    "# Main Loop with RAG Integration\n",
    "# =====================\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    context = ConversationContext(window_size=5)\n",
    "    generate_answer_signature = dspy.Signature(\"question -> answer\")\n",
    "    pot = ProgramOfThought(signature=generate_answer_signature)\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            user_message = input(\">>> \")\n",
    "\n",
    "            if user_message == ':exit':\n",
    "                break\n",
    "            elif user_message == ':history':\n",
    "                print(context.render())\n",
    "                continue\n",
    "\n",
    "            rag_context = retrieve_observing_metadata(user_message)\n",
    "            assistant_response = respond_cot(\n",
    "                context=context.render(),\n",
    "                message=user_message,\n",
    "                rag_context=rag_context\n",
    "            ).response\n",
    "\n",
    "            print(f'\\n<<< {assistant_response}\\n')\n",
    "\n",
    "            interaction = UserInteraction(message=user_message, response=assistant_response)\n",
    "            context.update(interaction)\n",
    "\n",
    "            if \"calculate\" in user_message.lower() or \"solve\" in user_message.lower():\n",
    "                variables = {\"gravity\": 9.81}\n",
    "                result = pot(question=user_message, variables=variables, context_str=context.render(), rag_context=rag_context)\n",
    "                print(\"ProgramOfThought Answer:\", result['answer'])\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            break\n",
    "\n",
    "    print('\\nBye!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aab3451",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T15:34:12.343386Z",
     "iopub.status.busy": "2025-01-08T15:34:12.343165Z",
     "iopub.status.idle": "2025-01-08T15:34:45.852487Z",
     "shell.execute_reply": "2025-01-08T15:34:45.851989Z",
     "shell.execute_reply.started": "2025-01-08T15:34:12.343370Z"
    }
   },
   "outputs": [],
   "source": [
    "import dspy\n",
    "import re\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "from dspy.primitives.python_interpreter import PythonInterpreter, CodePrompt\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# =====================\n",
    "# Logging and LM Configuration\n",
    "# =====================\n",
    "LOG = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "# Configure a local model with Ollama\n",
    "# lm = dspy.LM('ollama_chat/tulu3', api_base='https://xxxx-xxx-xx-xxx-xx.ngrok-free.app', api_key='') #api_key should be empty\n",
    "# lm = dspy.LM('ollama_chat/tulu3', api_base='http://localhost:11434', api_key='') #api_key should be empty\n",
    "\n",
    "# Read the API key from a file\n",
    "with open('/home/c/carlosm/.openaikey.txt', 'r') as file:\n",
    "    api_key = file.read().strip()\n",
    "\n",
    "# Configure the language model with OpenAI GPT-4o\n",
    "lm = dspy.LM('openai/gpt-4o', api_key=api_key)\n",
    "dspy.settings.configure(lm=lm)\n",
    "\n",
    "# =====================\n",
    "# Conversation Memory Classes\n",
    "# =====================\n",
    "\n",
    "class UserInteraction(BaseModel):\n",
    "    message: str = None\n",
    "    response: str = None\n",
    "\n",
    "    def serialize_by_role(self):\n",
    "        return [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"{self.message}\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": f\"{self.response}\"\n",
    "            }\n",
    "        ]\n",
    "\n",
    "\n",
    "class ConversationContext(BaseModel):\n",
    "    window_size: int = 3\n",
    "    content: list[UserInteraction] = []\n",
    "\n",
    "    @staticmethod\n",
    "    def _format_interaction(interaction):\n",
    "        return f\"User: {interaction.message}\\n\\nAssistant: {interaction.response}\\n\\n\"\n",
    "\n",
    "    def render(self):\n",
    "        formatted = [\n",
    "            self._format_interaction(interaction)\n",
    "            for interaction in self.content\n",
    "        ]\n",
    "        return \"\".join(formatted)\n",
    "\n",
    "    def update(self, interaction: UserInteraction):\n",
    "        # Keep only the last `window_size` interactions, then add the new one\n",
    "        self.content = self.content[-self.window_size:] + [interaction]\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.render()\n",
    "\n",
    "\n",
    "# =====================\n",
    "# Signature and ChainOfThought for Responding with Context\n",
    "# =====================\n",
    "\n",
    "class ResponseWithContext(dspy.Signature):\n",
    "    \"\"\"Respond to the user's message given some conversation context.\"\"\"\n",
    "    context = dspy.InputField(desc=\"The context of the conversation\")\n",
    "    message = dspy.InputField(desc=\"The user's message\")\n",
    "    rag_context = dspy.InputField(desc=\"Relevant data from observing metadata\")\n",
    "    response = dspy.OutputField(desc=\"A useful response to the user's message within the context of the conversation\")\n",
    "\n",
    "respond_cot = dspy.ChainOfThought(ResponseWithContext)\n",
    "\n",
    "# =====================\n",
    "# Retrieval Function (RAG)\n",
    "# =====================\n",
    "def getObservingData(dayObs=None):\n",
    "    \"\"\"Get the observing metadata for the current or a past day.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dayObs : `int`, optional\n",
    "        The day for which to get the observing metadata. If not specified,\n",
    "        the current day is used.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    observingData: `pandas.DataFrame`\n",
    "        The observing metadata for the specified day.\n",
    "    \"\"\"\n",
    "    currentDayObs = 20240904\n",
    "    if dayObs is None:\n",
    "        dayObs = currentDayObs\n",
    "    isCurrent = (dayObs == currentDayObs)\n",
    "\n",
    "    site = \"local\"  # Example site setting\n",
    "    if site == \"local\":\n",
    "        filename = f\"/home/c/carlosm/lsst/summit_extras/python/lsst/summit/extras/dayObs_{dayObs}.json\"\n",
    "    elif site in [\"rubin-devl\"]:\n",
    "        LOG.warning(\n",
    "            f\"Observing metadata at {site} is manually copied. It may not be updated in real-time.\"\n",
    "        )\n",
    "        filename = f\"/sdf/home/m/mfl/u/rubinTvDataProducts/sidecar_metadata/dayObs_{dayObs}.json\"\n",
    "    elif site in [\"staff-rsp\"]:\n",
    "        LOG.warning(\n",
    "            f\"Observing metadata at {site} is manually copied. It may not be updated in real-time.\"\n",
    "        )\n",
    "        filename = f\"/home/m/mfl/u/rubinTvDataProducts/sidecar_metadata/dayObs_{dayObs}.json\"\n",
    "    else:\n",
    "        raise RuntimeError(f\"Observing metadata not available for site: {site}\")\n",
    "\n",
    "    if not os.path.exists(filename):\n",
    "        LOG.warning(\n",
    "            f\"Observing metadata file for {'current ' if isCurrent else ''}dayObs \"\n",
    "            f\"{dayObs} does not exist at {filename}.\"\n",
    "        )\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    try:\n",
    "        table = pd.read_json(filename).T\n",
    "    except ValueError as e:\n",
    "        LOG.error(f\"Error reading JSON file {filename}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Remove columns starting with underscore (backend only)\n",
    "    table = table.drop([col for col in table.columns if col.startswith(\"_\")], axis=1, errors='ignore')\n",
    "    return table.sort_index()\n",
    "\n",
    "\n",
    "def retrieve_observing_metadata(query, dayObs=None):\n",
    "    \"\"\"Retrieve relevant observing data based on the query.\"\"\"\n",
    "    df = getObservingData(dayObs=dayObs)\n",
    "    if df.empty:\n",
    "        return \"No observing data available.\"\n",
    "    \n",
    "    # Filter or search the dataframe based on the query\n",
    "    result = df[df.apply(lambda row: query.lower() in row.to_string().lower(), axis=1)]\n",
    "    if result.empty:\n",
    "        return \"No relevant data found.\"\n",
    "    else:\n",
    "        return result.to_string(index=False)\n",
    "\n",
    "# =====================\n",
    "# ProgramOfThought Class (Updated for RAG)\n",
    "# =====================\n",
    "\n",
    "class ProgramOfThought(dspy.Module):\n",
    "    \"\"\"Class to generate and execute Python code iteratively to solve questions, now with context consideration.\"\"\"\n",
    "\n",
    "    def __init__(self, signature, max_iters=3):\n",
    "        super().__init__()\n",
    "        self.signature = signature\n",
    "        self.max_iters = max_iters\n",
    "        self.interpreter = PythonInterpreter(\n",
    "            action_space={\"print\": print},\n",
    "            import_white_list=[\"numpy\", \"astropy\", \"sympy\", \"matplotlib\"]\n",
    "        )\n",
    "\n",
    "    def forward(self, question, variables=None, context_str=\"\", rag_context=\"\"):\n",
    "        \"\"\"Generates and executes Python code for a given question.\"\"\"\n",
    "        for iteration in range(self.max_iters):\n",
    "            generated_code = self._generate_code(question, context_str, rag_context)\n",
    "            print(f\"\\nIteration {iteration + 1}:\")\n",
    "            print(\"Generated Code:\\n\", generated_code)\n",
    "\n",
    "            try:\n",
    "                result, _ = CodePrompt(generated_code).execute(\n",
    "                    self.interpreter,\n",
    "                    user_variable=variables\n",
    "                )\n",
    "                print(\"Execution Result:\\n\", result)\n",
    "                return {\"answer\": result}\n",
    "            except Exception as e:\n",
    "                LOG.error(f\"Execution Error: {e}\")\n",
    "\n",
    "        print(\"\\nFailed to generate valid code after several iterations.\")\n",
    "        return {\"answer\": None}\n",
    "\n",
    "    def _generate_code(self, question, context_str=\"\", rag_context=\"\"):\n",
    "        \"\"\"Generates Python code using the model with RAG data.\"\"\"\n",
    "        generate_code_signature = dspy.Signature(\"question -> generated_code\")\n",
    "        predict = dspy.Predict(generate_code_signature)\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "You are an AI that writes Python code to solve problems. \n",
    "Below is the conversation context and relevant observing metadata:\n",
    "\n",
    "Context:\n",
    "{context_str}\n",
    "\n",
    "Observing Metadata:\n",
    "{rag_context}\n",
    "\n",
    "Now, generate Python code to solve the following problem. \n",
    "Do not include any explanations, Markdown formatting, or comments. \n",
    "Only output the executable Python code.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Code:\n",
    "\"\"\"\n",
    "        completion = predict(question=prompt)\n",
    "        generated_code = completion.generated_code\n",
    "\n",
    "        # Remove any triple-backtick fences\n",
    "        cleaned_code = re.sub(r\"```[^\\n]*\\n\", \"\", generated_code)\n",
    "        cleaned_code = re.sub(r\"```\", \"\", cleaned_code)\n",
    "\n",
    "        return cleaned_code\n",
    "\n",
    "# =====================\n",
    "# Main Loop with RAG Integration\n",
    "# =====================\n",
    "if __name__ == '__main__':\n",
    "    context = ConversationContext(window_size=5)\n",
    "    generate_answer_signature = dspy.Signature(\"question -> answer\")\n",
    "    pot = ProgramOfThought(signature=generate_answer_signature)\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            user_message = input(\">>> \")\n",
    "\n",
    "            if user_message.strip().lower() == ':exit':\n",
    "                break\n",
    "            elif user_message.strip().lower() == ':history':\n",
    "                print(context.render())\n",
    "                continue\n",
    "\n",
    "            # Retrieve RAG context from local JSON\n",
    "            rag_context = retrieve_observing_metadata(user_message)\n",
    "\n",
    "            # Generate a short text response to the user\n",
    "            assistant_response = respond_cot(\n",
    "                context=context.render(),\n",
    "                message=user_message,\n",
    "                rag_context=rag_context\n",
    "            ).response\n",
    "\n",
    "            print(f'\\n<<< {assistant_response}\\n')\n",
    "\n",
    "            # Update conversation memory\n",
    "            interaction = UserInteraction(message=user_message, response=assistant_response)\n",
    "            context.update(interaction)\n",
    "\n",
    "            # If the user wants a calculation/solution, invoke ProgramOfThought\n",
    "            if \"calculate\" in user_message.lower() or \"solve\" in user_message.lower():\n",
    "                variables = {\"gravity\": 9.81}\n",
    "                result = pot(\n",
    "                    question=user_message,\n",
    "                    variables=variables,\n",
    "                    context_str=context.render(),\n",
    "                    rag_context=rag_context\n",
    "                )\n",
    "                print(\"ProgramOfThought Answer:\", result['answer'])\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            break\n",
    "\n",
    "    print('\\nBye!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3590256-0528-4538-b8f8-ac515fdf05c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
